---
title: "GBM 3ëŒ€ì¥ ì‹¤ìŠµ, Complex Matrix ì´ë¡ "
excerpt: "2022-11-23 XBG, LightGBM, Catboost, Complex Matrix"

# layout: post
categories:
  - TIL
tags:
  - python
  - EDA
  - Learning Machine
  - Feature Engineering
  - Linear Regression
  - Gradient Boosting
  - XBG
  - LightGBM
  - Catboost
  - Complex Matrix
spotifyplaylist: spotify/playlist/2KaQr0nx66AX399ZLLuTVf?si=43a48325c8fc4b16
---
### **âš ï¸ í•´ë‹¹ ë‚´ìš©ì€ ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼ AI School ì˜¤ëŠ˜ì½”ë“œ ë°•ì¡°ì€ ê°•ì‚¬ì˜ ìë£Œë¥¼ í† ëŒ€ë¡œ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.**
{% include spotifyplaylist.html id=page.spotifyplaylist %}

11/16ì—ì„œ ì´ì–´ì§

**ğŸ¤” ë°°ê¹…ê³¼ ë¶€ìŠ¤íŒ…ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°?**

- ë°°ê¹… : ì˜¤ë²„í”¼íŒ…
- ë¶€ìŠ¤íŒ… : ê°œë³„ íŠ¸ë¦¬ì˜ ì„±ëŠ¥ì´ ì¤‘ìš”í• ë•Œ

# Benz boosting model input

âš ï¸ **í•´ë‹¹ ê³¼ì •ì€ êµ¬ê¸€ì˜ colabì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤!** ë²„ì „ì˜ í˜¸í™˜ì„±ê³¼ ë”ë¶ˆì–´ boost ëª¨ë¸ë“¤ì€ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ì œì‘ë˜ìˆê¸° ë•Œë¬¸ì—, ë¡œì»¬í™˜ê²½ì—ì„œ ëª¨ë¸ì„ ì„¤ì¹˜í•  ê²½ìš° ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¸í•´ ë¡œì»¬í™˜ê²½ì— ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤.

## category type ë³€ê²½

ë²”ì£¼í˜• í”¼ì²˜ë“¤ì„ ë¨¼ì € ì‚´í´ë³´ê¸°ë¡œ í•œë‹¤.

```python
# object columnsë§Œ ë”°ë¡œ êº¼ë‚´ì„œ ë³€ìˆ˜í˜• í”¼ì²˜ë“¤ ë‹¤ë£¨ê¸°
cat_col = train.select_dtypes(include="object").columns
cat_col

ê²°ê³¼ê°’:
Index(['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8'], dtype='object')
```

```python
# lightGBM, CatBoostì—ì„œëŠ” ë²”ì£¼í˜• í”¼ì²˜ë¥¼ ì¸ì½”ë”©ì—†ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
# ë”°ë¡œ ë²”ì£¼í˜• í”¼ì²˜ë¥¼ ì§€ì •í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

train[cat_col] = train[cat_col].astype("category")
test[cat_col] = test[cat_col].astype("category")
```

## Feature Engineering

### One-Hot-Encoding

`pd.get_dummies()` ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ë„ ì¢‹ì§€ë§Œ, í˜„ì¬ ì˜ˆì œì—ì„œëŠ” sklearnì˜ ì„±ëŠ¥ì´ ë” ì¢‹ê²Œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— sklearnì„ ì‚¬ìš©í•œë‹¤.

```python
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(handle_unknown = "ignore")

train_ohe = ohe.fit_transform(train.drop(columns = 'y'))
test_ohe = transform(test)

train_ohe.shape, test_ohe.shape

ê²°ê³¼ê°’ :
((4209, 919), (376, 4209))
```

```python
# hold-out validation ìœ¼ë¡œ trainê°’ ë‚˜ëˆ„ê¸°

X = train_ohe
y = train.y

X.shape, y.shape

ê²°ê³¼ê°’ :
((4209, 919), (4209,))
```

## í•™ìŠµ, ê²€ì¦ì„¸íŠ¸ ë‚˜ëˆ„ê¸°

```python
# train_test_splitì„ ì´ìš©í•´ X, y ê°’ì„ X_train, X_valid, y_train, y_valid ìœ¼ë¡œ ë‚˜ëˆ ì¤ë‹ˆë‹¤.
# Hold-out-validationì„ ìœ„í•´ train, validì„¸íŠ¸ë¡œ ë‚˜ëˆ ì¤€ë‹¤.

from sklearn.model_selection import train_test_split

# test_size = 0.1 : valid ì‚¬ì´ì¦ˆë¥¼ ì§€ì •í•œë‹¤. 
	# trainì˜ ë¹„ì¤‘ì„ 90%, validì˜ ë¹„ì¤‘ì„ 10%ë¡œ ë‚˜ëˆ ì¤€ë‹¤.
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size = 0.1, random_state = 42
)

X_train.shape, X_valid.shape, y_train.shape, y_valid.shape

ê²°ê³¼ê°’:
((3788, 919), (421, 919), (3788,), (421,))

X_test = test_ohe
```

## **XGBoost ëª¨ë¸**

### XGBoost Parameter

- ë¶€ìŠ¤íŒ… íŒŒë¼ë¯¸í„°
    - Learning_rate[ê¸°ë³¸ê°’ : 0.3] : Learning rateê°€ ë†’ì„ìˆ˜ë¡ ê³¼ì í•©ë˜ê¸° ì‰¬ì›€
    - n_estimators [ê¸°ë³¸ê°’ : 100] : ìƒì„±í•  weaker learner ìˆ˜. learning_rateê°€ ë‚®ì„ ë• n_estimatorsë¥¼ ë†’ì—¬ì•¼ ê³¼ì í•©ì´ ë°©ì§€ë¨. valueê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ underfittingì´ ë˜ê³  ì´ëŠ” ë‚®ì€ ì •í™•ì„±ì˜ predictionì´ ë˜ëŠ” ë°˜ë©´, valueê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ overfittingì´ ë˜ê³  training data ì—ëŠ” ì •í™•í•œ predictionì„ ë³´ì´ì§€ë§Œ test dataì—ì„œëŠ” ì •í™•ì„±ì´ ë‚®ì€ predictionì„ ê°€ì§
    - max_depth [ê¸°ë³¸ê°’ : 3] : íŠ¸ë¦¬ì˜ maximum depth. ì ì ˆí•œ ê°’ì´ ì œì‹œë˜ì–´ì•¼ í•˜ê³  ë³´í†µ 3-10 ì‚¬ì´ ê°’ì´ ì ìš©ë¨, max_depthê°€ ë†’ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ì»¤ì ¸ ê³¼ì í•©ë˜ê¸° ì‰¬ì›€
    - min_child_weight [ê¸°ë³¸ê°’ : 1] : ê´€ì¸¡ì¹˜ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ í•©ì˜ ìµœì†Œë¥¼ ë§í•¨. ê°’ì´ ë†’ì„ìˆ˜ë¡ ê³¼ì í•©ì´ ë°©ì§€ë¨
    - gamma [ê¸°ë³¸ê°’ : 0] : ë¦¬í”„ë…¸ë“œì˜ ì¶”ê°€ë¶„í• ì„ ê²°ì •í•  ìµœì†Œì†ì‹¤ ê°ì†Œê°’. í•´ë‹¹ê°’ë³´ë‹¤ ì†ì‹¤ì´ í¬ê²Œ ê°ì†Œí•  ë•Œ ë¶„ë¦¬, ê°’ì´ ë†’ì„ìˆ˜ë¡ ê³¼ì í•©ì´ ë°©ì§€ë¨
    - subsample [ê¸°ë³¸ê°’ : 1] : weak learnerê°€ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨, ë³´í†µ 0.5 ~ 1 ì‚¬ìš©ë¨, ê°’ì´ ë‚®ì„ìˆ˜ë¡ ê³¼ì í•©ì´ ë°©ì§€ë¨
    - colsample_bytree [ ê¸°ë³¸ê°’ : 1 ] : ê° tree ë³„ ì‚¬ìš©ëœ featureì˜ í¼ì„¼í…Œì´ì§€, ë³´í†µ 0.5 ~ 1 ì‚¬ìš©ë¨, ê°’ì´ ë‚®ì„ìˆ˜ë¡ ê³¼ì í•©ì´ ë°©ì§€ë¨
- ì¼ë°˜ íŒŒë¼ë¯¸í„°
    - booster [ê¸°ë³¸ê°’ = gbtree] : ì–´ë–¤ ë¶€ìŠ¤í„° êµ¬ì¡°ë¥¼ ì“¸ì§€ ê²°ì •, ì˜ì‚¬ê²°ì •ê¸°ë°˜ëª¨í˜•(gbtree), ì„ í˜•ëª¨í˜•(gblinear), dart
    - n_jobs : XGBoostë¥¼ ì‹¤í–‰í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë³‘ë ¬ ìŠ¤ë ˆë“œ ìˆ˜
    - verbosity [ê¸°ë³¸ê°’ = 1] : ë¡œê·¸ì¶œë ¥ì—¬ë¶€ 0 (ë¬´ìŒ), 1 (ê²½ê³ ), 2 (ì •ë³´), 3 (ë””ë²„ê·¸)
    - early_stopping_rounds : ì†ì‹¤í•¨ìˆ˜ ê°’ì´ në²ˆì •ë„ ê°œì„ ì´ ì—†ìœ¼ë©´ í•™ìŠµì„ ì¤‘ë‹¨
- í•™ìŠµê³¼ì • íŒŒë¼ë¯¸í„°
    - eval_metric:
        - rmse: root mean square error
        - mae: mean absolute error
        - logloss: negative log-likelihood
        - error: Binary classification error rate (0.5 threshold)
        - merror: Multiclass classification error rate
        - mlogloss: Multiclass logloss
        - auc: Area under the curve
        - map (mean average precision)

```python
# xgboost ëª¨ë“ˆ í˜¸ì¶œ
import xgboost as xgb

model_xgb = xgb.XGBRegressor(random_state= 42, n_jobs=-1)

model_xgb.fit(X_train, y_train)

model_xgb.feature_importances_[:5]

ê²°ê³¼ê°’ : 
array([0.        , 0.00473197, 0.00115691, 0.        , 0.        ],
      dtype=float32)

xgb.plot_importance(model_xgb)
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled.png)

```python
xgb.plot_tree(model_xgb, num_trees=1)
fig = plt.gcf()
fig.set_size_inches(30, 20)
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled 1.png)

**ğŸ¤” ë°°ê¹… ëª¨ë¸ì€ ì‹œê°í™”ê°€ ì–´ë ¤ì›Œ 3rd party ë„êµ¬ë¥¼ ë”°ë¡œ ì„¤ì¹˜í•´ì•¼ ì‹œê°í™” ê°€ëŠ¥í•˜ë‹¤. ê·¸ê²ƒë„ ê°œë³„ íŠ¸ë¦¬ë¥¼ ì‹œê°í™” í•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤. ê·¸ëŸ°ë° ë¶€ìŠ¤íŒ… ëª¨ë¸ì€ ì™œ ì‹œê°í™”ê°€ ê°€ëŠ¥í• ê¹Œ?**

ë°°ê¹…ëª¨ë¸ì€ ë³‘ë ¬ì ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ìƒì„±í•˜ì§€ë§Œ, ë¶€ìŠ¤íŒ…ì€ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•˜ë‹¤.

valid ì˜ˆì¸¡ ì ìˆ˜ë¥¼ í™•ì¸í•´ë³´ë„ë¡ í•˜ì.

```python
# valid score
score_xgb = model_xgb.score(X_valid, y_valid)
score_xgb

ê²°ê³¼ê°’ : 0.6128264118065729
```

ì´í›„, í•™ìŠµëœ ë‚´ìš©ìœ¼ë¡œ ì˜ˆì¸¡ì„ ì‹œí‚¤ê³  ì œì¶œê¹Œì§€ ì§„í–‰í•˜ë„ë¡ í•œë‹¤. ì§„í–‰ ë°©ë²•ì€ ì§€ë‚œ í¬ìŠ¤íŒ…ì„ ì°¸ê³ í•˜ë„ë¡ í•œë‹¤.

```python
# predict
y_pred_xgb = model_xgb.predict(X_test)

submission['y'] = y_pred_xgb

file_name = f'{base_path}/sub_xgb_{score_xgb}.csv'
submission.to_csv(file_name)
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled 2.png)

## [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html) ëª¨ë¸

- XGBoostì— ë¹„í•´ ì„±ëŠ¥ì€ ë¹„ìŠ·í•˜ì§€ë§Œ í•™ìŠµ ì‹œê°„ì„ ë‹¨ì¶•ì‹œí‚¨ ëª¨ë¸ì´ë‹¤.
- XGBoostì— ë¹„í•´ ë” ì ì€ ì‹œê°„, ë” ì ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•œë‹¤.

### LightGBM Parameters

- max_depth : ë‚˜ë¬´ì˜ ê¹Šì´. ë‹¨ì¼ ê²°ì •ë‚˜ë¬´ì—ì„œëŠ” ì¶©ë¶„íˆ ë°ì´í„°ë¥¼ ê³ ë ¤í•˜ê¸° ìœ„í•´ depthë¥¼ ì ë‹¹í•œ ê¹Šì´ë¡œ ë§Œë“¤ì§€ë§Œ, ë³´ì •ë˜ê¸° ë•Œë¬¸ì— ë¶€ìŠ¤íŒ…ì—ì„œëŠ” ê¹Šì´ í•˜ë‚˜ì§œë¦¬ë„ ë§Œë“œëŠ” ë“±, ê¹Šì´ê°€ ì§§ì€ê²ƒì´ í¬ë¦¬í‹°ì»¬í•˜ì§€ ì•ŠìŒ
- min_data_in_leaf : ìì´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìµœì†Œ ë ˆì½”ë“œ ìˆ˜, ê¸°ë³¸ê°’ì€ 20, ê³¼ì í•©ì„ ë‹¤ë£¨ê¸° ìœ„í•´ ì‚¬ìš©
- feature_fraction : ë¶€ìŠ¤íŒ… ëŒ€ìƒ ëª¨ë¸ì´ ëœë¤í¬ë ˆìŠ¤íŠ¸ì¼ë•Œ, ëœë¤í¬ë ˆìŠ¤íŠ¸ëŠ” featureì˜ ì¼ë¶€ë§Œì„ ì„ íƒí•˜ì—¬ í›ˆë ¨í•˜ëŠ”ë°, ì´ë¥¼ í†µì œí•˜ê¸° ìœ„í•œ íŒŒë¼ë¯¸í„°, 0.8ì´ë¼ë©´ LightGBMì´ ê° ë°˜ë³µì—ì„œ 80%ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ íŠ¸ë¦¬ë¥¼ ìƒì„±
- bagging_fraction : ë°ì´í„°ì˜ ì¼ë¶€ë§Œì„ ì‚¬ìš©í•˜ëŠ” baggingì˜ ë¹„ìœ¨. ì˜ˆë¥¼ë“¤ì–´ ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë°ì´í„°ì˜ ì¼ë¶€ë§Œì„ ê°€ì ¸ì™€ì„œ í›ˆë ¨ì‹œí‚¤ëŠ”ë°, ì´ëŠ” ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•˜ë©° ì•½í•œì˜ˆì¸¡ê¸°ë¥¼ ëª¨ë‘ í•©ì¹ ê²½ìš°ëŠ” ì˜¤íˆë ¤ ì˜ˆì¸¡ì„±ëŠ¥ì´ ì¢‹ì•„ì§ˆ ìˆ˜ ìˆìŒ í›ˆë ¨ ì†ë„ë¥¼ ë†’ì´ê³  ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë° ì‚¬ìš©
- early_stopping_round : ë”ì´ìƒ validationë°ì´í„°ì—ì„œ ì •í™•ë„ê°€ ì¢‹ì•„ì§€ì§€ ì•Šìœ¼ë©´ ë©ˆì¶°ë²„ë¦¼ í›ˆë ¨ë°ì´í„°ëŠ” ê±°ì˜ ì—ëŸ¬ìœ¨ì´ 0ì— ê°€ê¹ê²Œ ì¢‹ì•„ì§€ê¸° ë§ˆë ¨ì¸ë°, validationë°ì´í„°ëŠ” í›ˆë ¨ì— ì‚¬ìš©ë˜ì§€ ì•Šê¸°ë•Œë¬¸ì— ì¼ì •ì´ìƒ ì¢‹ì•„ì§€ì§€ ì•Šê¸° ë•Œë¬¸
- lambda : ì •ê·œí™”ì— ì‚¬ìš©ë˜ëŠ” íŒŒë¼ë¯¸í„°, ì¼ë°˜ì ì¸ ê°’ì˜ ë²”ìœ„ëŠ” 0 ~ 1
- min_gain_to_split : ë¶„ê¸°ê°€ ë˜ëŠ” ìµœì†Œ ì •ë³´ì´ë“, íŠ¸ë¦¬ì—ì„œ ìœ ìš©í•œ ë¶„í•  ìˆ˜ë¥¼ ì œì–´í•˜ëŠ” ë° ì‚¬ìš©
- max_cat_group : ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ë§ìœ¼ë©´, í•˜ë‚˜ë¡œ í‰ì³ì„œ ì²˜ë¦¬í•˜ê²Œë” ë§Œë“œëŠ” ìµœì†Œë‹¨ìœ„
- objective : lightgbmì€ regression, binary, multiclass ëª¨ë‘ ê°€ëŠ¥
- boosting: gbdt(gradient boosting decision tree), rf(random forest), dart(dropouts meet multiple additive regression trees), goss(Gradient-based One-Side Sampling)
- num_leaves: ê²°ì •ë‚˜ë¬´ì— ìˆì„ ìˆ˜ ìˆëŠ” ìµœëŒ€ ìì‚¬ê·€ ìˆ˜. ê¸°ë³¸ê°’ì€ 0.31
- learning_rate : ê° ì˜ˆì¸¡ê¸°ë§ˆë‹¤ì˜ í•™ìŠµë¥  learning_rateì€ ì•„ë˜ì˜ num_boost_roundì™€ë„ ë§ì¶°ì£¼ì–´ì•¼ í•¨
- num_boost_round : boostingì„ ì–¼ë§ˆë‚˜ ëŒë¦´ì§€ ì§€ì •í•œë‹¤. ë³´í†µ 100ì •ë„ë©´ ë„ˆë¬´ ë¹ ë¥´ê²Œ ëë‚˜ë©°, ì‹œí—˜ìš©ì´ ì•„ë‹ˆë©´ 1000ì •ë„ ì„¤ì •í•˜ë©°, early_stopping_roundê°€ ì§€ì •ë˜ì–´ìˆìœ¼ë©´ ë”ì´ìƒ ì§„ì „ì´ ì—†ì„ ê²½ìš° ì•Œì•„ì„œ ë©ˆì¶¤
- device : gpu, cpu
- metric: lossë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•œ ê¸°ì¤€. mae (mean absolute error), mse (mean squared error), ë“±
- max_bin : ìµœëŒ€ bin
- categorical_feature : ë²”ì£¼í˜• ë³€ìˆ˜ ì§€ì •
- ignore_column : ì»¬ëŸ¼ì„ ë¬´ì‹œí•œë‹¤. ë¬´ì‹œí•˜ì§€ ì•Šì„ê²½ìš° ëª¨ë‘ trainingì— ë„£ëŠ”ë°, ë­”ê°€ ë‚¨ê²¨ë†“ì•„ì•¼í•  ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì„¤ì •
- save_binary: True ë©”ëª¨ë¦¬ ì ˆì•½

ì „ì²´ì ì¸ ê³¼ì •ì€ XGBoostì™€ ë™ì¼í•˜ë‚˜, ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ë§Œ LightGBMìœ¼ë¡œ ë‹¤ë¥´ë‹¤. ëª¨ë¸ë§Œ ë°”ê¿”ì¤€ ë’¤ ì§„í–‰í•˜ë„ë¡ í•œë‹¤.

```python
import lightgbm as lgbm
# model_lgbm
model_lgbm = lgbm.LGBMRegressor(random_state = 42, n_jobs = -1)

model_lgbm.fit(X_train, y_train)

lgbm.plot_importance(model_lgbm)
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled 3.png)

```python
lgbm.plot_tree(model_lgbm, figsize=(20, 20), tree_index=0, 
               show_info=['split_gain', 'internal_value', 'internal_count', 'leaf_count'])
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled 4.png)

```python
# valid score
score_lgbm = model_lgbm.score(X_valid, y_valid)

score_lgbm 
ê²°ê³¼ê°’ : 0.5720514617008872

# predict
y_pred_lgbm = model_lgbm.predict(X_test)

y_pred_lgbm[:5]
ê²°ê³¼ê°’ :
array([ 76.99591393,  92.18224812,  77.30829539,  75.78294519,
       111.97681237])

# submit
submission['y'] = y_pred_lgbm
file_name = f'{base_path}/sub_lgbm_{score_lgbm}.csv'
submission.to_csv(file_name)
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled 5.png)

## [CatBoost](https://catboost.ai/en/docs/references/training-parameters/common) ëª¨ë¸

- catboostëŠ” ê¸°ì¡´ GBTì˜ ëŠë¦° í•™ìŠµ ì†ë„ì™€ ê³¼ëŒ€ì í•© ë¬¸ì œë¥¼ ê°œì„ í•œ ëª¨ë¸ì…ë‹ˆë‹¤.
- ê³¼ëŒ€ì í•©ì´ë€ ëª¨ë¸ì´ ì§€ë‚˜ì¹œ í•™ìŠµìœ¼ë¡œ ì¸í•´ ê²½í–¥ì´ í•™ìŠµìš© ì„¸íŠ¸ì— ì ë ¤ ìˆëŠ” í˜„ìƒì„ ë§í•©ë‹ˆë‹¤.
- í•™ìŠµìš© ì„¸íŠ¸ì—ì„œëŠ” ì˜ˆì¸¡ì„ ì˜ í•˜ì§€ë§Œ(íŠ¹ìˆ˜í•œ ìƒí™©), ì¼ë°˜ì ì¸ ìƒí™©ì—ì„œ ì˜ˆì¸¡ ëŠ¥ë ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì…ë‹ˆë‹¤.

### ì£¼ìš” íŒŒë¼ë¯¸í„°

- cat_features
    - ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ë±ìŠ¤ ê°’
- loss_function
    - ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
- eval_metric
    - í‰ê°€ ë©”íŠ¸ë¦­ì„ ì§€ì •í•©ë‹ˆë‹¤.
- iterations
    - ë¨¸ì‹ ëŸ¬ë‹ ì¤‘ ë§Œë“¤ì–´ì§ˆ ìˆ˜ ìˆëŠ” íŠ¸ë¦¬ì˜ ìµœëŒ€ ê°¯ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
- learning_rate
    - ë¶€ìŠ¤íŒ… ê³¼ì • ì¤‘ í•™ìŠµë¥ ì„ ì§€ì •í•©ë‹ˆë‹¤.
- subsample
    - ë°°ê¹…ì„ ìœ„í•œ ì„œë¸Œìƒ˜í”Œ ë¹„ìœ¨ì„ ì§€ì •í•©ë‹ˆë‹¤.
- max_leaves
    - ìµœì¢… íŠ¸ë¦¬ì˜ ìµœëŒ€ ë¦¬í”„ ê°œìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.

Catboost ì—­ì‹œ ë‹¤ë¥¸ GBMëª¨ë¸ì²˜ëŸ¼ ì§„í–‰ì„ í•´ì¤€ë‹¤.

```python
# catboost
import catboost

# model_cat

# catboost ì˜ íšŒê·€ ì•Œê³ ë¦¬ì¦˜ì˜ ê¸°ë³¸ metric ì€ RMSE
# eval_metric = "R2" : ì•Œê³ ë¦¬ì¦˜ì„ R2 Scoreë¡œ ì„¤ì •.
model_cat = catboost.CatBoostRegressor(eval_metric="R2", verbose = False)

from scipy.stats import randint
from sklearn.utils.fixes import loguniform

# catboostëŠ” ìì²´ì ìœ¼ë¡œ searchCVê¸°ëŠ¥ì´ ìˆë‹¤.
# grow_policy : íŠ¸ë¦¬ë¥¼ ì–´ë–¤ì‹ìœ¼ë¡œ ì„±ì¥ì‹œí‚¬ ê²ƒì¸ì§€ë¥¼ ê²°ì •. Defalutë¡œëŠ” SymmetricTree(ëŒ€ì¹­íŠ¸ë¦¬).
                # Lossguide - ë¦¬í”„ë³„, Depthwise : ê¹Šì´ë³„
param_grid = {
    'n_estimators': randint(100, 300),
    'depth': randint(1, 5),
    'learning_rate': loguniform(1e-3, 0.1),
    'min_child_samples': randint(10, 40),
    'grow_policy': ['SymmetricTree', 'Lossguide', 'Depthwise']
}

# randomized_search
result = model_cat.randomized_search(param_grid, X_train, y_train, cv=3, n_iter=10)

df_result = pd.DataFrame(result)
df_result = df_result.loc[["train-R2-mean", "test-R2-mean"], "cv_results"]
df_result

ê²°ê³¼ê°’ :
train-R2-mean    [-55.46630687885133, -49.38772650148917, -43.9...
test-R2-mean     [-55.529821058885176, -49.444705355730996, -44...
Name: cv_results, dtype: object

pd.DataFrame({"train-R2-mean": df_result.loc["train-R2-mean"], 
              "test-R2-mean" :  df_result.loc["test-R2-mean"] }).tail(3)
```

|  | train-R2-mean | test-R2-mean |
| --- | --- | --- |
| 215 | 0.616273 | 0.554339 |
| 216 | 0.616432 | 0.554440 |
| 217 | 0.616722 | 0.554476 |

```python
# R2 Scoreì˜ ë§ˆì§€ë§‰ 50ê°œ ê°’ì„ plot

pd.DataFrame({"train-R2-mean": df_result.loc["train-R2-mean"], 
              "test-R2-mean" :  df_result.loc["test-R2-mean"] }).tail(50).plot()
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled 6.png)

```python
# R2 Scoreì˜ ì „ì²´ì ì¸ ëª¨ì–‘ plot
pd.DataFrame({"train-R2-mean": df_result.loc["train-R2-mean"], 
              "test-R2-mean" :  df_result.loc["test-R2-mean"] }).plot()
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled 7.png)

```python
# fit
model_cat.fit(X_train, y_train)

score_cat = model_cat.score(X_valid, y_valid)
score_cat
ê²°ê³¼ê°’ : 0.6172756661736007

# Predict
y_cat_pred = model_cat.predict(X_test)

# submit 
submission['y'] = y_cat_pred
file_name = f'{base_path}/sub_cat_{score_cat}.csv'
submission.to_csv(file_name)
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled 8.png)

## ë²”ì£¼í˜• ë°ì´í„° ë‹¤ë£¨ê¸°

### **category type ë³€ê²½**

```python
# object => category
# category íƒ€ì…ìœ¼ë¡œ ë˜ì–´ìˆìœ¼ë©´ lightGBM, CatBoostì—ì„œ ì¸ì½”ë”©ì—†ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

cat_col = train.select_dtypes(include="object").columns
train[cat_col] = train[cat_col].astype("category")
test[cat_col] = test[cat_col].astype("category")
```

### LightGBM

```python
# lgbm.LGBMRegressor
model_lgbmr = lgbm.LGBMRegressor(random_state = 42)

# ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ì§€ ì•Šê³  category í˜•íƒœë¡œ ë„£ì–´ì£¼ë©´ ì•Œì•„ì„œ í•™ìŠµí•œë‹¤.
# categoryí˜•íƒœë¡œ ë˜ì–´ ìˆë‹¤ë©´ ì¸ì½”ë”© ê³¼ì •ì´ í•„ìš” ì—†ì–´ì§„ë‹¤.

from sklearn.model_selection import cross_val_score

cv_score_lgbmr = cross_val_score(model_lgbmr, train.drop(columns="y"), train["y"], cv=3)

# fit & predict
model_lgbmr.fit(train.drop(columns = 'y'), train['y'])
```

### Catboost

```python
model_cat = catboost.CatBoostRegressor(eval_metric='R2', verbose=False, cat_features=cat_col.tolist())

from sklearn.model_selection import cross_val_predict

y_valid_cat = cross_val_predict(model_cat, train.drop(columns = "y"), train["y"], cv = 3)

from sklearn.metrics import r2_score

r2_score(train["y"], y_valid_cat)

# fit & predict 
y_pred_cat = model_cat.fit(train.drop(columns="y"), train["y"]).predict(test)
```

ì´ ê³¼ì •ì„ ì§„í–‰í•œ ì´ìœ ?

 â‡’ ì „ì²˜ë¦¬, ì¸ì½”ë”© ì—†ì´ ì‰½ê²Œ í•™ìŠµì„ ì§„í–‰í•´ë³´ê¸° ìœ„í•¨.

# ë¶ˆê· í˜• ë°ì´í„°: SMOTE ì™€ ë¶„ë¥˜ ì¸¡ì •ì§€í‘œ

## Confusion Matrix(í˜¼ë™ í–‰ë ¬)

- Confusion Matrixì˜ ì‚¬ìš© ì´ìœ 
    - ì•”í™˜ì ì§„ë‹¨ì˜ ê²½ìš°, ë†’ì€ ì •í™•ë„ì™€ ìƒê´€ì—†ì´ ì°¸/ê±°ì§“ì´ êµ‰ì¥íˆ ì¤‘ìš”í•œ ë°ì´í„° ì¢…ë¥˜ ì¤‘ í•˜ë‚˜
    - í¬ì†Œí•œ ë°ì´í„°ë¥¼ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´, ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„ë³´ë‹¤ ì¤‘ìš”í•  ê²½ìš°ê°€ ìˆìŒ
    - ë˜í•œ, í˜„ì‹¤ì—ì„œ ë§ˆì£¼í•˜ëŠ” ë°ì´í„°ë“¤ì˜ ëŒ€ë¶€ë¶„ì€ â€˜ë¶ˆê· í˜• ë°ì´í„°â€™ì„.
    - ì •í™•ë„ ì™¸ì— ë‹¤ë¥¸ ì¸¡ì • ì§€í‘œê°€ í•„ìš”

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled 9.png)

ìœ„ ê·¸ë¦¼ì€, ì¶œì €ë§ˆë‹¤ ë‹¤ë¦„.

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled 10.png)

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-23-benz_complex/Untitled 11.png)

- TRUE : ëª¨ë¸ì´ ë§ì·„ì„ ë•Œ
- FALSE : ëª¨ë¸ì´ í‹€ë¦¼
- Positive : ëª¨ë¸ì´ ì˜ˆì¸¡ ê°’ì´ TRUE
- Negative : ëª¨ë¸ì´ ì˜ˆì¸¡ ê°’ì´ FALSE
- FP(False Positive, Negative Positive) - 1ì¢… ì˜¤ë¥˜
    - ì‹¤ì œëŠ” ì„ì‹ ì´ ì•„ë‹Œë°(0), ì„ì‹ (1)ë¡œ ì˜ˆì¸¡
- FN(False Negative, Positive Negative) - 2ì¢… ì˜¤ë¥˜
    - ì‹¤ì œëŠ” ì„ì‹ ì¸ë°(1), ì„ì‹ ì´ ì•„ë‹Œ ê²ƒ(0)ìœ¼ë¡œ ì˜ˆì¸¡
- $Precision = tp/(tp + fp)$
    - ì •ë°€ë„ ì¸¡ì • - ìŠ¤íŒ¸ë©”ì¼ í™•ì¸ ì—¬ë¶€
- $Recall = tp/(tp + fn)$
    - ì¬í˜„ìœ¨ - ì•”í™˜ì ì—¬ë¶€

ğŸ˜µâ€ğŸ’« **ê°œë…ì´ í–‡ê°ˆë¦°ë‹¤?** 

ëª¨ë¥´ëŠ”ê±¸ ì•„ëŠ”ê±°ë¡œ ê±°ì§“ë§ í–ˆë‹¤ : ëª¨ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆê¹ ê±°ì§“ë§ í•˜ì§€ ë§ë¼ê³  1ë‹¨ê³„ ì •ë„ë¡œ í˜¼ë‚¨(1ì¢… ì˜¤ë¥˜)

ì•„ëŠ”ë° ëª¨ë¥´ëŠ”ê±°ë¡œ ê±°ì§“ë§ í–ˆë‹¤ : ì˜ë„ì ìœ¼ë¡œ ê±°ì§“ë§ì„ í–ˆê¸°ì— 2ë‹¨ê³„ í˜¸ë˜ê²Œ í˜¼ë‚¨(2ì¢… ì˜¤ë¥˜)