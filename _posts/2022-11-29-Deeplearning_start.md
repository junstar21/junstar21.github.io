---
title: "ë”¥ëŸ¬ë‹ ì…ë¬¸"
excerpt: "2022-11-29 Deep Learning"

# layout: post
categories:
  - TIL
tags:
  - python
  - Deep Learning
  - category
  - loss
  - activation
  - optimizer
spotifyplaylist: spotify/playlist/2KaQr0nx66AX399ZLLuTVf?si=43a48325c8fc4b16
---
### **âš ï¸ í•´ë‹¹ ë‚´ìš©ì€ ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼ AI School ì˜¤ëŠ˜ì½”ë“œ ë°•ì¡°ì€ ê°•ì‚¬ì˜ ìë£Œë¥¼ í† ëŒ€ë¡œ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.**

[ì§€ë‚œ í¬ìŠ¤íŠ¸](https://junstar21.github.io/til/complex_deep/)

# [ì‹ ê²½ë§ì´ë€ ë¬´ì—‡ì¸ê°€?](https://www.youtube.com/watch?v=aircAruvnKk&ab_channel=3Blue1Brown)

- Dropout : ê³¼ëŒ€ì í•©(ê³¼ì í•©, ì˜¤ë²„í”¼íŒ…)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì¼ë¶€ ë…¸ë“œë¥¼ ì œê±°í•˜ê³  ì‚¬ìš©í•˜ëŠ” ê²ƒ.

# **[í…ì„œí”Œë¡œ 2.0 ì‹œì‘í•˜ê¸°: ì´ˆë³´ììš©](https://www.tensorflow.org/tutorials/quickstart/beginner)**

## Tensorflow

```python
# Tensorflow í˜¸ì¶œ
import tensorflow as tf
```

## **ë°ì´í„°ì„¸íŠ¸(**[MNIST ë°ì´í„°ì„¸íŠ¸](http://yann.lecun.com/exdb/mnist/)) **ë¡œë“œí•˜ê¸°**

```python
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
# 255ë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ëŠ” 0~255 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§„ ë³€ìˆ˜ë“¤ì„ 0~1 ì‚¬ì´ë¡œ ìŠ¤ì¼€ì¼ë§ í•˜ê¸° ìœ„í•¨
# tfì—ì„œëŠ” XëŠ” ì†Œë¬¸ìë¥¼ ì‚¬ìš©
x_train, x_test = x_train / 255.0, x_test / 255.0

# ndim : ì°¨ì›ì˜ ìˆ˜
x_train.ndim, x_test.ndim

ê²°ê³¼ê°’ : (3, 3)
```

0ë²ˆì§¸ íŒŒì¼ì„ ì‹œê°í™”í•´ë³´ë„ë¡ í•˜ì.

```python
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

idx = 0
# DataFrameì— ìˆ˜ì¹˜ì— ë”°ë¼ì„œ ìƒ‰ì„ ì…íŒë‹¤.
display(pd.DataFrame(x_train[idx]).style.background_gradient())
sns.heatmap(x_train[idx], cmap = "gray")
plt.title(f"label : [idx]")
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled.png)

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled 1.png)

ìˆ«ì 5ë¼ëŠ” ì´ë¯¸ì§€ê°€ ë“¤ì–´ê°€ìˆë‹¤.

ğŸ¤” **MNIST ì†ê¸€ì”¨ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì€ ì™œ ë§Œë“¤ì—ˆì„ê¹Œ?**

ìš°í¸ë²ˆí˜¸ë¥¼ ì½ì–´ë‚´ê°€ ìœ„í•´ì„œ ë§Œë“¤ì–´ì§€ê²Œ ë˜ì—ˆë‹¤. [MNIST ë°ì´í„°ë² ì´ìŠ¤ - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „ (wikipedia.org)](https://ko.wikipedia.org/wiki/MNIST_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4)

## **ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ ë¹Œë“œí•˜ê¸°**

ì¸µì„ ì°¨ë¡€ëŒ€ë¡œ ìŒ“ì•„Â `tf.keras.Sequential`ëª¨ë¸ì„ ë§Œë“ ë‹¤. í›ˆë ¨ì— ì‚¬ìš©í•  ì˜µí‹°ë§ˆì´ì €(optimizer)ì™€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì„ íƒí•œë‹¤.

```python
model = tf.keras.models.Sequential([
# Flatten : nì°¨ì›ì˜ ë°ì´í„°ë¥¼ 1ì°¨ì›ìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.
#           ì—¬ê¸°ì„œëŠ” 28*28ì˜ 2ì°¨ì› ë°ì´í„°ë¥¼ 1ì°¨ì›ìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.
  tf.keras.layers.Flatten(input_shape=(28, 28)),
# output = activation(dot(input, kernel) + bias)
# ì¶œë ¥ = í™œì„±í™”í•¨ìˆ˜(í–‰ë ¬ê³±(input, kernel) + í¸í–¥)
# 128 : hidden layerì˜ unit(nod)ì˜ ê°œìˆ˜
  tf.keras.layers.Dense(128, activation='relu'), 
# dropout(0.2) : unitì˜ 20%ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

# optimizer : ì˜¤ì°¨ê°€ ìµœì†Œê°€ ë˜ëŠ” ì§€ì ì„ ì°¾ê¸° ìœ„í•œ í•¨ìˆ˜. ê¸°ìš¸ê¸°, ë°©í–¥, learning rateë¥¼ ê³ ë ¤
# optimizerëŠ” ëŒ€ë¶€ë¶„ adamì„ ì‚¬ìš©. ëª¨ë¥´ë©´ adamì„ ì¨ë„ ë¬´ë°©í•¨.
model.compile(optimizer='adam',
# loss : ì†ì‹¤ìœ¨ì„ ì¸¡ì •
              loss='sparse_categorical_crossentropy',
# metrics : í‰ê°€ì§€í‘œ
              metrics=['accuracy'])
```

outputì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ì•„ë˜ ì´ë¯¸ì§€ë¥¼ ë³´ë©´ ëœë‹¤. (ì¶œì²˜ : [But what is a neural network? | Chapter 1, Deep learning - YouTube](https://www.youtube.com/watch?v=aircAruvnKk&ab_channel=3Blue1Brown))

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled 2.png)

ì´ì œ ë¹Œë“œí•œ ëª¨ë¸ë¡œ ì˜ˆì¸¡ì„ ì§„í–‰í•˜ë„ë¡ í•œë‹¤.

```python
predictions = model(x_train[:1]).numpy()
predictions

ê²°ê³¼ê°’ : 
array([[0.09023821, 0.16924651, 0.09414785, 0.09464978, 0.11364367,
0.04734194, 0.06211803, 0.08623113, 0.0585422 , 0.18384069]],
dtype=float32)
```

```python
import numpy as np
# softmax ëŠ” ë‹¤ ë”í–ˆì„ ë•Œ 1ì´ ëœë‹¤.
# softmax ëŠ” ê° í´ë˜ìŠ¤ì˜ í™•ë¥ ì„ ì¶œë ¥í•œë‹¤.

smax = tf.nn.softmax(predictions).numpy()
smax, f"softmaxëŠ” ë‹¤ ë”í–ˆì„ ë•Œ 1ì´ ë©ë‹ˆë‹¤. : {np.sum(smax)}", 
# np.argmax() : ()ì•ˆì— ê°€ì¥ í° ê°’ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•´ì¤Œ
f"ì •ë‹µ í´ë˜ìŠ¤ : {np.argmax(smax)}"

ê²°ê³¼ê°’:
(array([[0.09893701, 0.10707095, 0.09932458, 0.09937444, 0.10127999,
0.09478272, 0.09619364, 0.09854135, 0.09585028, 0.10864502]],
dtype=float32), 'softmaxëŠ” ë‹¤ ë”í–ˆì„ ë•Œ 1ì´ ë©ë‹ˆë‹¤. : 1.0', 'ì •ë‹µ í´ë˜ìŠ¤ : 9')
```

ì˜ˆì¸¡í•œ ê°’ì„ ì†ì‹¤í•¨ìˆ˜ë¥¼ í†µí•´ì„œ ì˜¤ì°¨ê°€ ì–¼ë§ˆë‚˜ ë°œìƒí•˜ëŠ”ì§€ë¥¼ í‰ê°€í•´ë³´ë„ë¡ í•œë‹¤.

```python
# loss : ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì´ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ìˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” ì†ì‹¤í•¨ìˆ˜
# ë¶„ë¥˜ëŠ” ì£¼ë¡œ í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ë¥¼ ì‚¬ìš©í•œë‹¤.
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
```

```python
loss_fn(y_train[:1], predictions).numpy()

ê²°ê³¼ê°’ : 2.356168
```

## **ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€í•˜ê¸°**

ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  í‰ê°€ë¥¼ ì§„í–‰í•˜ë„ë¡ í•œë‹¤.

```python
# ì—¬ëŸ¬ ë²ˆ í•™ìŠµì„ í•˜ë©´ lossê°€ ì ì  ì¤„ì–´ë“¤ê²Œ ëœë‹¤.
# í•™ìŠµì„ í•˜ë©´ì„œ weigth, bias ê°’ì„ ì—…ë°ì´íŠ¸ í•œë‹¤.
# epochs : í•´ë‹¹ ëª¨ë¸ì„ ëª‡ë²ˆ í›ˆë ¨í• ì§€ë¥¼ ì •í•œë‹¤.
model.fit(x_train, y_train, epochs=5)

ê²°ê³¼ê°’:
Epoch 1/5
1875/1875 [==============================] - 6s 3ms/step - loss: 0.2978 - accuracy: 0.9123
Epoch 2/5
1875/1875 [==============================] - 6s 3ms/step - loss: 0.1448 - accuracy: 0.9567
Epoch 3/5
1875/1875 [==============================] - 6s 3ms/step - loss: 0.1075 - accuracy: 0.9675
Epoch 4/5
1875/1875 [==============================] - 6s 3ms/step - loss: 0.0892 - accuracy: 0.9722
Epoch 5/5
1875/1875 [==============================] - 7s 4ms/step - loss: 0.0761 - accuracy: 0.9754
<keras.callbacks.History at 0x7f9e2344b090>
```

tfì—ì„œ í•™ìŠµì€ ëˆ„ì ì´ ëœë‹¤. lossë¥¼ ì¤„ì´ê³  accuracyë¥¼ ë†’ì´ê¸° ìœ„í•´ì„œ í•œë²ˆ ë” ì§„í–‰í•´ë³´ë„ë¡ í•œë‹¤.

```python
model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)

ê²°ê³¼ê°’:
Epoch 1/5
1875/1875 [==============================] - 5s 3ms/step - loss: 0.0659 - accuracy: 0.9789
Epoch 2/5
1875/1875 [==============================] - 5s 3ms/step - loss: 0.0585 - accuracy: 0.9811
Epoch 3/5
1875/1875 [==============================] - 5s 3ms/step - loss: 0.0533 - accuracy: 0.9826
Epoch 4/5
1875/1875 [==============================] - 5s 3ms/step - loss: 0.0496 - accuracy: 0.9837
Epoch 5/5
1875/1875 [==============================] - 5s 3ms/step - loss: 0.0444 - accuracy: 0.9856
313/313 - 1s - loss: 0.0704 - accuracy: 0.9809 - 609ms/epoch - 2ms/step
[0.070419542491436, 0.98089998960495]
```

ëª¨ë¸ì´ í™•ë¥ ì„ ë°˜í™˜í•˜ë„ë¡ í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í›ˆë ¨ëœ ëª¨ë¸ì„ ë˜í•‘í•˜ê³  ì—¬ê¸°ì— ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì²¨ë¶€í•  ìˆ˜ ìˆë‹¤.

```python
probability_model = tf.keras.Sequential([
  model,
  tf.keras.layers.Softmax()
])

y_pred = probability_model(x_test[:5])
y_pred

ê²°ê³¼ê°’:
<tf.Tensor: shape=(5, 10), dtype=float32, numpy=
array([[0.08533826, 0.08533826, 0.08533835, 0.08534597, 0.08533826,
0.08533826, 0.08533826, 0.23194528, 0.08533826, 0.08534084],
[0.08533677, 0.08533696, 0.23196885, 0.08533677, 0.08533677,
0.08533677, 0.08533677, 0.08533677, 0.08533677, 0.08533677],
[0.08533906, 0.23193273, 0.08534102, 0.08533909, 0.08533926,
0.08533906, 0.08533906, 0.08535206, 0.08533961, 0.08533906],
[0.2319687 , 0.08533678, 0.08533681, 0.08533678, 0.08533678,
0.08533678, 0.08533701, 0.08533679, 0.08533678, 0.08533678],
[0.08536852, 0.08536851, 0.08536855, 0.08536851, 0.23146583,
0.08536851, 0.08536852, 0.08537116, 0.08536851, 0.08558335]],
dtype=float32)>
```

```python
print(np.argmax(y_pred[0]))
sns.heatmap(x_test[0], cmap = "gray")
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled 3.png)

# [ê¸°ë³¸ ë¶„ë¥˜ : ì˜ë¥˜ ì´ë¯¸ì§€ ë¶„ë¥˜](https://www.tensorflow.org/tutorials/keras/classification)

- ìš´ë™í™”ë‚˜ ì…”ì¸  ê°™ì€ ì˜· ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸ì„ í›ˆë ¨
- ì™„ì „í•œ í…ì„œí”Œë¡œ(TensorFlow) í”„ë¡œê·¸ë¨ì„ ë¹ ë¥´ê²Œ ì‚´í´ ë³´ë„ë¡ í•œë‹¤.
- ìƒì„¸í•œ ë‚´ìš©ì„ ì´í•´í•˜ì§€ ëª»í•˜ë”ë¼ê³  ê´œì°®ë‹¤.

```python
# TensorFlow and tf.keras
import tensorflow as tf

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

ê²°ê³¼ê°’ : 2.9.2
```

## íŒ¨ì…˜ MNIST ë°ì´í„°ì…‹ ì„í¬íŠ¸í•˜ê¸°

- 10ê°œì˜ ë²”ì£¼(category)ì™€ 70,000ê°œì˜ í‘ë°± ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœÂ [íŒ¨ì…˜ MNIST](https://github.com/zalandoresearch/fashion-mnist) ë°ì´í„°ì…‹ì„ ì‚¬ìš©
- ì´ë¯¸ì§€ëŠ” í•´ìƒë„(28x28 í”½ì…€)ê°€ ë‚®ê³  [ë‹¤ìŒ](https://tensorflow.org/images/fashion-mnist-sprite.png)ì²˜ëŸ¼ ê°œë³„ ì˜· í’ˆëª©ì„ ë‚˜íƒ€ë‚¸ë‹¤.

```python
# ë°ì´í„° ë¡œë“œ
fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
```

- `train_images`ì™€Â `train_labels`Â ë°°ì—´ì€ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ”Â *í›ˆë ¨ ì„¸íŠ¸*
- `test_images`ì™€Â `test_labels`Â ë°°ì—´ì€ ëª¨ë¸ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©ë˜ëŠ”Â *í…ŒìŠ¤íŠ¸ ì„¸íŠ¸*

```python
# ë…ë¦½ ë³€ìˆ˜ ì„¤ì •
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
```

## ë°ì´í„° íƒìƒ‰

```python
train_images.shape

ê²°ê³¼ê°’ : (60000, 28, 28)

len(train_labels)

ê²°ê³¼ê°’ : 60000

train_labels
ê²°ê³¼ê°’ : array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)
```

ë ˆì´ë¸”ì€ 0 ê³¼ 9 ì‚¬ì´ì˜ ì •ìˆ˜ì„ì„ í™•ì¸í•˜ì˜€ë‹¤.

```python
test_images.shape

ê²°ê³¼ê°’ : (10000, 28, 28)

len(test_labels)

ê²°ê³¼ê°’ : 10000
```

## ë°ì´í„° ì „ì²˜ë¦¬

```python
# min~max ë²”ìœ„ í™•ì¸
train_images[0].min(), train_images[0].max()

ê²°ê³¼ê°’ : (0, 255)
```

```python
# class_namesê°€ ë¬´ì—‡ì¸ì§€ í™•ì¸
class_names[train_labels[0]]

ê²°ê³¼ê°’ : Ankle boot
```

ì´ë¯¸ì§€ë¥¼ ì‹œê°í™”í•´ë³´ë„ë¡ í•˜ì.

```python
plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled 4.png)

```python
# 0~1ë¡œ ì •ê·œí™” í•´ì£¼ê¸°
train_images = train_images / 255.0

test_images = test_images / 255.0
```

**í›ˆë ¨Â ì„¸íŠ¸**ì—ì„œÂ ì²˜ìŒÂ 25ê°œÂ ì´ë¯¸ì§€ì™€Â ê·¸Â ì•„ë˜Â í´ë˜ìŠ¤Â ì´ë¦„ì„Â ì¶œë ¥í•´Â ë³´ì£ .Â ë°ì´í„°Â í¬ë§·ì´Â ì˜¬ë°”ë¥¸ì§€Â í™•ì¸í•˜ê³ Â ë„¤íŠ¸ì›Œí¬Â êµ¬ì„±ê³¼Â í›ˆë ¨í• Â ì¤€ë¹„ë¥¼Â ë§ˆì¹©ë‹ˆë‹¤.

```python
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled 5.png)

## ëª¨ë¸ êµ¬ì„±

### ì¸µ êµ¬ì„±

```python
# units = Nod = Neuron
# ì…ë ¥ - ì€ë‹‰ì¸µ - ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±ëœ ë„¤íŠ¸ì›Œí¬ ëª¨ë¸
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])
```

### ëª¨ë¸ ì»´íŒŒì¼

```python
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
```

## ëª¨ë¸ í›ˆë ¨

í›ˆë ¨ì€ ê¸°ì¡´ ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ ì§„í–‰ëœë‹¤.

```python
model.fit(train_images, train_labels, epochs=10)
```

ì´í›„ ë‚´ìš©ë“¤ì€ ìœ„ ê³¼ì •ê³¼ ìƒì´í•˜ê¸°ì— ìƒëµí•˜ë„ë¡ í•œë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://www.tensorflow.org/tutorials/keras/classification)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

## ìš”ì•½

- ë‹¤ë¥¸ ëª¨ë¸ì— ì ìš©í•œë‹¤ë©´ ì¸µ êµ¬ì„±ì„ ì–´ë–»ê²Œ í• ê²ƒì¸ê°€? 
â‡’ì…ë ¥-ì€ë‹‰-ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±
- ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ê°’ì´ ë¶„ë¥˜(ì´ì§„, ë©€í‹°í´ë˜ìŠ¤), íšŒê·€ì¸ì§€ì— ë”°ë¼ ì¶œë ¥ì¸µ êµ¬ì„±, loss ì„¤ì •ì´ ë‹¬ë¼ì§„ë‹¤.
- ë¶„ë¥˜, íšŒê·€ì— ë”°ë¼ ì¸¡ì • ì§€í‘œ ì •í•˜ê¸°
- í™œì„±í™”í•¨ìˆ˜ëŠ” reluë¥¼ ì‚¬ìš©, optimizer ë¡œëŠ” adamì„ ì‚¬ìš©í•˜ë©´ baseline ì •ë„ì˜ ìŠ¤ì½”ì–´ê°€ ë‚˜ì˜¨ë‹¤.
- fitì„ í•  ë•Œ epochë¥¼ í†µí•´ ì—¬ëŸ¬ ë²ˆ í•™ìŠµì„ ì§„í–‰í•˜ëŠ”ë° ì´ ë•Œ, epochìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ëŒ€ì²´ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì§€ë§Œ ê³¼ëŒ€ì í•©(ì˜¤ë²„í”¼íŒ…)ì´ ë  ìˆ˜ë„ ìˆë‹¤.
- epochìˆ˜ê°€ ë„ˆë¬´ ì ë‹¤ë©´ ê³¼ì†Œì í•©(ì–¸ë”í”¼íŒ…)ì´ ë  ìˆ˜ë„ ìˆë‹¤.

# Optimizer - ë°ì´í„°ì™€ ì†ì‹¤í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ ì—…ë°ì´íŠ¸ ë˜ëŠ” ë°©ì‹

- ëª¨ë¸ì´ ì¸ì‹í•˜ëŠ” ë°ì´í„°ì™€ í•´ë‹¹ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ ì—…ë°ì´íŠ¸ë˜ëŠ” ë°©ì‹ì„ ëœ»í•¨

## [ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)](https://ko.wikipedia.org/wiki/%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95)

ì†ì‹¤í•¨ìˆ˜ì˜ ì†ì‹¤ì´ ë‚®ì•„ì°ŒëŠ” ìª½ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ë©° ì›€ì§ì´ë©° ìµœì†Ÿê°’ì„ ì°¾ëŠ” ë°©ë²•

## í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•

- ê²½ì‚¬í•˜ê°•ë²•ê³¼ ë‹¤ë¥´ê²Œ ëœë¤í•˜ê²Œ ì¶”ì¶œí•œ ì¼ë¶€ ë°ì´í„°ì— ëŒ€í•´ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ë©° ìµœì ì˜ í•´ë¥¼ ì°¾ìŒ.
- ì†ë„ê°€ ë” ë¹ ë¥´ë‚˜, ì •í™•ë„ê°€ ë‚®ìŒ(local minima ë¬¸ì œ)

## ê·¹ì†Œ(local minima) ë¬¸ì œ

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled 6.png)

- ì¶œì²˜ : [ê·¹ê°’ - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „ (wikipedia.org)](https://ko.wikipedia.org/wiki/%EA%B7%B9%EA%B0%92)
- ì˜µí‹°ë§ˆì´ì €ê°€ ìµœì†Œì (global minimum)ì„ ì°¾ì•„ì•¼í•˜ëŠ”ë° ìµœì†Œì ì´ ì•„ë‹Œ ê·¹ì†Œì (local minimum)ì„ ì°¾ëŠ” ë¬¸ì œê°€ ë°œìƒ.

## Optimizer

- ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ **ê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ **í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
- ë“±ì‚°ìœ¼ë¡œ ë¹„ìœ 
    - Gradient Descent : ë‚´ë ¤ê°€ëŠ” ë°©í–¥ì„ ì°¾ëŠ” ë°©ë²•
    - Optimizer : íš¨ìœ¨ì (ì‹œê°„, ì„±ëŠ¥ ê³ ë ¤)ìœ¼ë¡œ íƒìƒ‰
- ì°¸ê³ í• ë§Œ ìë£Œ : [ììŠµí•´ë„ ëª¨ë¥´ê² ë˜ á„ƒá…µá†¸á„…á…¥á„‚á…µá†¼, ë¨¸ë¦¬ì†ì— ì¸ìŠ¤í†¨ ì‹œì¼œë“œë¦½ë‹ˆë‹¤. (slideshare.net)](https://www.slideshare.net/yongho/ss-79607172)

# Pima TF classification

ê³¼ê±° ë‹¹ë‡¨ë³‘ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ì„œ TFë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤.(ì°¨í›„ ì—…ë¡œë“œë¥¼ ì§„í–‰í•  ì˜ˆì •) ë”°ë¼ì„œ ì‚¬ì „ ë°ì´í„° íƒìƒ‰ ê³¼ì •ì€ ìƒëµí•˜ë„ë¡ í•œë‹¤.

## ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°

```python
# label_name
label_name = 'Outcome'

# X, y ë§Œë“¤ê¸°
feature_names = df.columns.drop(label_name)

X = df[feature_names]
y = df[label_name]

# sklearn.model_selection ìœ¼ë¡œ ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

ê²°ê³¼ê°’ : ((614, 8), (154, 8), (614,), (154,))
```

```python
# tensorflow ë¥¼ tfë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
import tensorflow as tf
```

## **í™œì„±í™” í•¨ìˆ˜ activations**

í™œì„±í™” í•¨ìˆ˜ëŠ” ë§ì€ ì¢…ë¥˜ë“¤ì´ ìˆë‹¤. ì–´ë–¤ ì¢…ë¥˜ë“¤ì´ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•´ë³´ì.

```python
print(dir(tf.keras.activations)[10:]

ê²°ê³¼ê°’ : ['deserialize', 'elu', 'exponential', 'gelu', 'get', 'hard_sigmoid', 'linear', 'relu', 'selu', 'serialize', 'sigmoid', 'softmax', 'softplus', 'softsign', 'swish', 'tanh']
```

ê° í™œì„±í™” í•¨ìˆ˜ë“¤ì´ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ì‹œê°í™”ë¥¼ í†µí•´ì„œ ì•Œì•„ë³´ì.

```python
# tf.keras.activations.sigmoid(x)
# xì¶•ì€ ì›ë˜ ê°’ì„ yì¶•ì€ sigmoid í•¨ìˆ˜ë¥¼ í†µê³¼ì‹œí‚¨ ê°’ì…ë‹ˆë‹¤. 
plt.plot(x, tf.keras.activations.sigmoid(x), linestyle='--', label="sigmoid")
plt.axvline(0) 
plt.legend()
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled 7.png)

```python
# tanh
plt.plot(x, tf.keras.activations.tanh(x), linestyle='--', label="tanh")
plt.axvline(0) 
plt.legend()
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled 8.png)

```python
# swish
plt.plot(x, tf.keras.activations.swish(x), linestyle='--', label="swish")
plt.axvline(0) 
plt.legend()
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled 9.png)

```python
# relu
plt.plot(x, tf.keras.activations.relu(x), linestyle='--', label="relu")
plt.axvline(0) 
plt.legend()
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled 10.png)

## ë”¥ëŸ¬ë‹ ë ˆì´ì–´ ë§Œë“¤ê¸°

### ëª¨ë¸ ë¹Œë”©

```python
# ì…ë ¥ë°ì´í„° ìˆ˜ êµ¬í•˜ê¸°
input_shape = X.shape[1]
input_shape

ê²°ê³¼ê°’ : 8
```

```python
# tf.keras.models.Sequential ë¡œ ì…ë ¥-íˆë“ -ì¶œë ¥(sigmoid) ë ˆì´ì–´ë¡œ êµ¬ì„±
model = tf.keras.models.Sequential([
# í•´ë‹¹ ë°ì´í„°ëŠ” 1ì°¨ì›ìœ¼ë¡œ flattenì„ í•´ì¤„ í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì—(ì´ì§„ë¶„ë¥˜) Denseë¥¼ ì ìš©ì‹œì¼œì¤€ë‹¤.
  tf.keras.layers.Dense(128, input_shape=[input_shape]),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1, activation='sigmoid')
])
```

### ëª¨ë¸ ì»´íŒŒì¼

```python
model.compile(optimizer='adam',
# ì´ì§„ ë¶„ë¥˜ì´ê¸° ë•Œë¬¸ì— loss funcitonì€ binary_crossentropyë¥¼ ì‚¬ìš©í•œë‹¤.
              loss='binary_crossentropy',
              metrics=['accuracy'])
```

## í•™ìŠµ

- ë°°ì¹˜(batch): ëª¨ë¸ í•™ìŠµì— í•œ ë²ˆì— ì…ë ¥í•  ë°ì´í„°ì…‹
- ì—í­(epoch): ëª¨ë¸ í•™ìŠµì‹œ ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµí•œ íšŸìˆ˜
- ìŠ¤í…(step): (ëª¨ë¸ í•™ìŠµì˜ ê²½ìš°) í•˜ë‚˜ì˜ ë°°ì¹˜ë¥¼ í•™ìŠµí•œ íšŸìˆ˜

```python
# í•™ìŠµê³¼ì •ì„ ì¶œë ¥í•˜ëŠ” ê³¼ì •ì„ '.'ìœ¼ë¡œ í‘œí˜„í•´ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤. 
class PrintDot(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        if epoch % 100 == 0: print('')
        print('.', end='')

# val_loss ê¸°ì¤€ìœ¼ë¡œ ê°’ì˜ í–¥ìƒì´ ì—†ë‹¤ë©´ ë©ˆì¶”ê²Œ í•œë‹¤.
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
```

```python
from re import VERBOSE
# í•™ìŠµí•˜ê¸°
# callbacks=[early_stop, PrintDot()]
history = model.fit(X_train, y_train, epochs = 100, validation_split=0.2, callbacks = [early_stop, PrintDot()], verbose
```

```python
# í•™ìŠµê²°ê³¼ì˜ history ê°’ì„ ê°€ì ¸ì™€ì„œ ë¹„êµí•˜ê¸° ìœ„í•´ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
df_hist = pd.DataFrame(history.history)
df_hist.tail()
```

|  | loss | accuracy | val_loss | val_accuracy |
| --- | --- | --- | --- | --- |
| 11 | 0.553114 | 0.729124 | 0.501547 | 0.813008 |
| 12 | 0.531260 | 0.731161 | 0.521571 | 0.723577 |
| 13 | 0.546065 | 0.723014 | 0.497894 | 0.747967 |
| 14 | 0.539685 | 0.716904 | 0.520553 | 0.715447 |
| 15 | 0.565220 | 0.698574 | 0.507848 | 0.756098 |

í•´ë‹¹ í•™ìŠµê°™ì€ ê²½ìš°, 15ë²ˆë§Œì— í•™ìŠµì´ ì¢…ë£Œë˜ì—ˆë‹¤. ì—¬ëŸ¬ê°€ì§€ ì´ìœ ê°€ ìˆì§€ë§Œ, ìš°ì„  early_stopì—ì„œ patienceë¥¼ ë„ˆë¬´ ì ê²Œ ì¤€ ê²ƒìœ¼ë¡œ íŒë‹¨ëœë‹¤. 100ì •ë„ ë¶€ì—¬í•œë‹¤ë©´ í•™ìŠµì„ ë” ëŠ˜ë¦´ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•  ìˆ˜ ìˆìœ¼ë‚˜ ê³¼ëŒ€ì í•© ë°œìƒì„ ìœ ì˜í•´ì•¼ í•œë‹¤.

### í•™ìŠµê²°ê³¼ ì‹œê°í™”

```python
# loss, accuracy, val_loss ê°’ ì‹œê°í™” 
df_hist[["loss", "accuracy", "val_loss"]].plot()
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-29-Deeplearning_start/Untitled 11.png)