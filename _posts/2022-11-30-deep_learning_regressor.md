---
title: "ë”¥ëŸ¬ë‹: íšŒê·€"
excerpt: "2022-11-30 Deep Learning: Regression"

# layout: post
categories:
  - TIL
tags:
  - python
  - Deep Learning
  - regression
  - loss
  - activation
  - optimizer
spotifyplaylist: spotify/playlist/2KaQr0nx66AX399ZLLuTVf?si=43a48325c8fc4b16
---
### **âš ï¸ í•´ë‹¹ ë‚´ìš©ì€ ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼ AI School ì˜¤ëŠ˜ì½”ë“œ ë°•ì¡°ì€ ê°•ì‚¬ì˜ ìë£Œë¥¼ í† ëŒ€ë¡œ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.**

[ì§€ë‚œ í¬ìŠ¤íŠ¸](https://junstar21.github.io/til/Deeplearning_start/)


# Pima TF classification

## ì˜ˆì¸¡

```python
y_pred = model.predict(X_test)
y_pred
```

ìœ„ ì½”ë“œëŒ€ë¡œ í•˜ë©´ ì„¸ë¡œë¡œ ì½”ë“œê°€ ë§¤ìš° ê¸¸ê²Œ ë‚˜ì˜¨ë‹¤. ì¢€ë” ë³´ê¸° ì¢‹ì€ ì‹œê°í™”ë¥¼ í•˜ê¸° ìœ„í•´ `flatten()`ì„ ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤.

```python
# ì˜ˆì¸¡ê°’ ì‹œê°í™”
# ì„ê³„ê°’ì„ ì •í•´ì„œ íŠ¹ì •ê°’ ì´ìƒì´ë©´ True, ì•„ë‹ˆë©´ Falseë¡œ ë³€í™˜í•´ì„œ ì‚¬ìš©í•  ì˜ˆì •.
# Precision-Recall Tradeoffì˜ thresholdë¥¼ ì ìš©ì‹œì¼œì¤€ë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤.
y_predict = (y_pred.flatten() > 0.5).astype(int)
y_predict
```

![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled.png)

flattení•œ ë°ì´í„°ë¥¼ í† ëŒ€ë¡œ ì •í™•ë„ë¥¼ ì§ì ‘ ê³„ì‚°í•´ë³´ì

```python
(y_test == y_predict).mean()
```

ğŸ’¡ ì‘ì„±í•œ ëª¨ë¸ë“¤ì„ tweekí•´ì„œ ëª¨ë¸ì„ ë‹¤ì–‘í•˜ê²Œ êµ¬ì„±í•´ì„œ ì ìˆ˜ í–¥ìƒì„ ë…¸ë ¤ë³´ë„ë¡ í•˜ì.

- dropoutì€ layerë§ˆë‹¤ ì–¼ë§ˆë‚˜ ë–¨ì–´ëœ¨ë¦´ ê²ƒì¸ì§€ë¥¼ ì •í•œë‹¤. ì‚¬ìš©í•œ layerê°€ ì—¬ëŸ¬ ê°œë©´ dropoutì„ í•´ë‹¹ layer ë°”ë¡œ ë°‘ì— ì¶”ê°€ì ìœ¼ë¡œ ì‘ì„±í•´ë„ ëœë‹¤.
- loss => W(ê°€ì¤‘ì¹˜), b(í¸í–¥) ê°’ì„ ì—…ë°ì´íŠ¸ í•´ì£¼ê¸° ìœ„í•´ ì‚¬ìš©í•˜ê³ , metric ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€
    - ë¶„ë¥˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” loss :
        - ì´ì§„ë¶„ë¥˜ - binarycrossentropy
        ë‹¤ì¤‘ë¶„ë¥˜ : ì›í•«ì¸ì½”ë”© - categorical_crossentropy
        ë‹¤ì¤‘ë¶„ë¥˜ : ì˜¤ë””ë„ - sparse_categorical_crossentropy

**ğŸ¤” ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì„ í–ˆì„ ë•Œ ì–´ëŠìª½ì´ ì„±ëŠ¥ì´ ë” ì¢‹ì•˜ëŠ”ê°€?**

ë¹„ìŠ·í•˜ë©° í° ì°¨ì´ë¥¼ ëŠë¼ì§€ ëª»í–ˆë‹¤. ê·¸ ì´ìœ ëŠ” ë³´í†µì€ ì •í˜•ë°ì´í„°ëŠ” ë”¥ëŸ¬ë‹ë³´ë‹¤ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì´ ëŒ€ì²´ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ë•Œê°€ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë¬´ì—‡ë³´ë‹¤ë„ ì¤‘ìš”í•œ ê²ƒì€ ë°ì´í„° ì „ì²˜ë¦¬ì™€ í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ì´ ì„±ëŠ¥ì— ë” ë§ì€ ì˜í–¥ì„ ì£¼ê²Œ ëœë‹¤.

âš ï¸**garbage in garbage out => ì¢‹ì€ ë°ì´í„°ë¥¼ í†µí•´ ëª¨ë¸ì„ ë§Œë“œëŠ”ê²Œ ì„±ëŠ¥ì— ê°€ì¥ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤!**

# **[ê¸°ë³¸ íšŒê·€: ì—°ë¹„ ì˜ˆì¸¡](https://www.tensorflow.org/tutorials/keras/regression)**

ì•„ë˜ ì‹¤ìŠµì€ êµ¬ê¸€ì˜ Tensorflow íŠœí† ë¦¬ì–¼ ë¬¸ì„œ ì¤‘ íšŒê·€ íŒŒíŠ¸ì´ë‹¤. ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ë ¤ë©´ ìƒë‹¨ ì œëª©ì„ í´ë¦­í•˜ë©´ ì´ë™í•œë‹¤. ì •ê·œí™” ì „ ê³¼ì •ì€ í•´ë‹¹ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ë„ë¡ í•˜ì.

## **ì •ê·œí™”**

```python
train_dataset.describe().transpose()[['mean', 'std']]
```

|  | mean | std |
| --- | --- | --- |
| MPG | 23.310510 | 7.728652 |
| Cylinders | 5.477707 | 1.699788 |
| Displacement | 195.318471 | 104.331589 |
| Horsepower | 104.869427 | 38.096214 |
| Weight | 2990.251592 | 843.898596 |
| Acceleration | 15.559236 | 2.789230 |
| Model Year | 75.898089 | 3.675642 |
| Europe | 0.178344 | 0.383413 |
| Japan | 0.197452 | 0.398712 |
| USA | 0.624204 | 0.485101 |

### ì •ê·œí™” ë ˆì´ì–´

kerasì—ì„œëŠ” ì •ê·œí™”ë¥¼ í•´ì£¼ëŠ” ëª…ë ¹ì–´ê°€ ìˆë‹¤.

```python
normalizer = tf.keras.layers.Normalization(axis=-1)
```

ê·¸ëŸ° ë‹¤ìŒÂ `Normalization.adapt`ë¥¼ í˜¸ì¶œí•˜ì—¬ ì „ì²˜ë¦¬ ë ˆì´ì–´ì˜ ìƒíƒœë¥¼ ë°ì´í„°ì— ë§ì¶˜ë‹¤.

```python
normalizer.adapt(np.array(train_features))
```

```python
# í‰ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•˜ê³  ë ˆì´ì–´ì— ì €ì¥
print(normalizer.mean.numpy())

ê²°ê³¼ê°’:
[[   5.478  195.318  104.869 2990.252   15.559   75.898    0.178    0.197
     0.624]]
```

```python
# ë ˆì´ì–´ê°€ í˜¸ì¶œë˜ë©´ ê° íŠ¹ì„±ì´ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™”ëœ ì…ë ¥ ë°ì´í„°ë¥¼ ë°˜í™˜

first = np.array(train_features[:1])

with np.printoptions(precision=2, suppress=True):
  print('First example:', first)
  print()
  print('Normalized:', normalizer(first).numpy())
```

## ì„ í˜•íšŒê·€

- ì‹¬ì¸µ ì‹ ê²½ë§ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ì „ì— í•˜ë‚˜ ë° ì—¬ëŸ¬ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì„ í˜• íšŒê·€ë¶€í„° ì‹œì‘í•œë‹¤.

### **í•˜ë‚˜ì˜ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•œ ì„ í˜• íšŒê·€**

- ë‹¨ì¼ ë³€ìˆ˜ ì„ í˜• íšŒê·€ë¡œ ì‹œì‘í•˜ì—¬Â `'Horsepower'`ì—ì„œÂ `'MPG'`ë¥¼ ì˜ˆì¸¡
- `tf.keras`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ êµìœ¡í•  ë•ŒëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•œë‹¤.Â [ì¼ë ¨ì˜ ë‹¨ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ”](https://www.tensorflow.org/guide/keras/sequential_model)Â `tf.keras.Sequential`ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤.
    - `tf.keras.layers.Normalization`Â ì „ì²˜ë¦¬ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬Â `'Horsepower'`Â ì…ë ¥ íŠ¹ì„±ì„ ì •ê·œí™” í•œë‹¤.
    - ì„ í˜• ë³€í™˜()ì„ ì ìš©í•˜ì—¬ ì„ í˜• ë ˆì´ì–´(`tf.keras.layers.Dense`)ë¡œ 1ê°œì˜ ì¶œë ¥ì„ ìƒì„±í•œë‹¤.
        
        y=mx+b
        
- *ì…ë ¥*ì˜ ìˆ˜ëŠ”Â `input_shape`ì¸ìˆ˜ë¡œ ì„¤ì •í•˜ê±°ë‚˜ ëª¨ë¸ì´ ì²˜ìŒ ì‹¤í–‰ë  ë•Œ ìë™ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.

ë¨¼ì €Â `'Horsepower'`íŠ¹ì„±ìœ¼ë¡œ êµ¬ì„±ëœ NumPy ë°°ì—´ì„ ë§Œë“  í›„,Â `tf.keras.layers.Normalization`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ìƒíƒœë¥¼Â `horsepower`ë°ì´í„°ì— ë§ì¶˜ë‹¤.

```python
# ì‚¬ì´í‚·ëŸ°ì˜ ì „ì²˜ë¦¬ ê¸°ëŠ¥ì´ë‚˜ ì§ì ‘ ê³„ì‚°ì„ í†µí•´ ì •ê·œí™”ë¥¼ í•´ì£¼ëŠ” ë°©ë²•ë„ ìˆë‹¤.
# TFì—ì„œë„ ì •ê·œí™” í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.
# horsepower_normalizer : Horsepower ë³€ìˆ˜ë¥¼ ê°€ì ¸ì™€ì„œ í•´ë‹¹ ë³€ìˆ˜ë§Œ ì •ê·œí™”

horsepower = np.array(train_features['Horsepower'])

horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)
horsepower_normalizer.adapt(horsepower)
```

Keras ìˆœì°¨ ëª¨ë¸ ë¹Œë“œí•œë‹¤.

```python
# ì „ì²˜ë¦¬ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•´ì„œ ëª¨ë¸ì„ ë§Œë“¤ ë•Œ ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ ê°™ì´ ë„£ì–´ ì¤„ ìˆ˜ ìˆë‹¤.
# ì¥ì  : ì •ê·œí™” ë°©ë²•ì„ ëª¨ë¥´ë”ë¼ë„ ì¶”ìƒíšŒëœ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ì„œ ì‰½ê²Œ ì •ê·œí™” í•  ìˆ˜ ìˆë‹¤.
# ë‹¨ì  : ì†ŒìŠ¤ì½”ë“œ, ë¬¸ì„œë¥¼ ì—´ì–´ë³´ê¸° ì „ì—ëŠ” ì¶”ìƒí™”ëœ ê¸°ëŠ¥ì´ ì–´ë–¤ ê¸°ëŠ¥ì¸ì§€ ì•Œê¸° ì–´ë µë‹¤.
# ì‚¬ì´í‚·ëŸ°ì˜ pipeline ê¸°ëŠ¥ê³¼ ìœ ì‚¬.
# í™œì„±í•¨ìˆ˜ë¥¼ ì ì§€ ì•Šìœ¼ë©´ 'linear(y = x)'ê°€ defalutë¡œ ì ìš©ëœë‹¤.

horsepower_model = tf.keras.Sequential([
    horsepower_normalizer,
    layers.Dense(units=1)
])

horsepower_model.summary()
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 1.png)

**ğŸ¤” ë ˆì´ì–´ êµ¬ì„±ì—ì„œ ì¶œë ¥ì¸µì˜ ë¶„ë¥˜ì™€ íšŒê·€ì˜ ì°¨ì´?**

- ë¶„ë¥˜ëŠ”(n, activation='softmax'), (1, activation='sigmoid') ì´ ìˆëŠ”ë° íšŒê·€ëŠ” í•­ë“±í•¨ìˆ˜
    - í•­ë“±í•¨ìˆ˜ : ì…ë ¥ë°›ì€ ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
- íšŒê·€ì˜ ì¶œë ¥ì¸µì€ í˜„ì¬ê¹Œì§€ ê³„ì‚°ëœ ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— í•­ë“±í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ê¹Œì§€ ê³„ì‚°ëœ ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥
- íšŒê·€ì˜ ì¶œë ¥ì¸µì€ í•­ìƒ `layers.Dense(units=1)`í˜•íƒœë¡œ ë‚˜ì˜¨ë‹¤.

KerasÂ `Model.fit`ì„ ì‚¬ìš©í•˜ì—¬ 100 epochì— ëŒ€í•œ í›ˆë ¨ì„ ì‹¤í–‰í•œë‹¤.

ì´ ëª¨ë¸ì€Â `'Horsepower'`ë¡œë¶€í„°Â `'MPG'`ë¥¼ ì˜ˆì¸¡í•œë‹¤. ì²˜ìŒ 10ê°œì˜ 'Horsepower' ê°’ì— ëŒ€í•´ í›ˆë ¨ë˜ì§€ ì•Šì€ ëª¨ë¸ì„ ì‹¤í–‰í•œë‹¤. ê²°ê³¼ëŠ” ì¢‹ì§€ ì•Šì§€ë§Œ ì˜ˆìƒë˜ëŠ” ëª¨ì–‘Â `(10, 1)`ì„ ê°€ì§€ê³  ìˆë‹¤.

```python
horsepower_model.predict(horsepower[:10])

ê²°ê³¼ê°’:
1/1 [==============================] - 0s 104ms/step
array([[ 0.769],
       [ 0.434],
       [-1.419],
       [ 1.078],
       [ 0.975],
       [ 0.383],
       [ 1.155],
       [ 0.975],
       [ 0.254],
       [ 0.434]], dtype=float32)
```

ëª¨ë¸ì´ ë¹Œë“œë˜ë©´Â `Model.compile`ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ì ˆì°¨ë¥¼ êµ¬ì„±í•œë‹¤. compileì— ê°€ì¥ ì¤‘ìš”í•œ ì¸ìˆ˜ëŠ”Â `loss`ë°Â `optimizer`ì´ë‹¤. ì´ë“¤ì´ ìµœì í™” ëŒ€ìƒ(`mean_absolute_error)`ê³¼ ë°©ì‹(`tf.keras.optimizers.Adam`ì‚¬ìš©)ì„ ì •ì˜í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

```python
horsepower_model.compile(
    # Adam(learning_rate = 0.1) : ë¶„ë¥˜ì™€ ë‹¤ë¥´ê²Œ learning rateë¥¼ ì„¤ì •í•´ì„œ ê²½ì‚¬í•˜ê°•ë²•ì˜ í•˜ê°•ì„ ì¡°ì ˆ
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')
```

```python
%%time
history = horsepower_model.fit(
    train_features['Horsepower'],
    train_labels,
    epochs=100,
    # Suppress logging.
    verbose=0,
    # Calculate validation results on 20% of the training data.
    validation_split = 0.2)
```

`history`ê°ì²´ì— ì €ì¥ëœ í†µê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ í›ˆë ¨ ì§„í–‰ ìƒí™©ì„ ì‹œê°í™”í•œë‹¤.

```python
hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()
```

|  | loss | val_loss | epoch |
| --- | --- | --- | --- |
| 95 | 3.803493 | 4.186892 | 95 |
| 96 | 3.804063 | 4.181873 | 96 |
| 97 | 3.804071 | 4.199119 | 97 |
| 98 | 3.805533 | 4.185143 | 98 |
| 99 | 3.803304 | 4.187895 | 99 |

```python
def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  plt.ylim([0, 10])
  plt.xlabel('Epoch')
  plt.ylabel('Error [MPG]')
  plt.legend()
  plt.grid(True)
```

```python
plot_loss(history)
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 2.png)

ë‚˜ì¤‘ì„ ìœ„í•´ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•œë‹¤.

```python
test_results = {}

test_results['horsepower_model'] = horsepower_model.evaluate(
    test_features['Horsepower'],
    test_labels, verbose=0)
```

ë‹¨ì¼ ë³€ìˆ˜ íšŒê·€ì´ë¯€ë¡œ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì…ë ¥ì˜ í•¨ìˆ˜ë¡œ ì‰½ê²Œ ë³¼ ìˆ˜ ìˆë‹¤.

```python
# linspace : xê°’ì„ ì„ì˜ë¡œ ìƒì„±
x = tf.linspace(0.0, 250, 251)
y = horsepower_model.predict(x)

def plot_horsepower(x, y):
  plt.scatter(train_features['Horsepower'], train_labels, label='Data')
  plt.plot(x, y, color='k', label='Predictions')
  plt.xlabel('Horsepower')
  plt.ylabel('MPG')
  plt.legend()

plot_horsepower(x, y)
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 3.png)

## **ë‹¤ì¤‘ ì…ë ¥ì´ ìˆëŠ” ì„ í˜• íšŒê·€**

ì´ì „ì— ì •ì˜í•˜ê³  ì „ì²´ ë°ì´í„°ì„¸íŠ¸ì— ì ìš©í•œÂ `normalizer`(`tf.keras.layers.Normalization(axis=-1)`
)ì˜ ì²« ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ 2ë‹¨ê³„ Keras Sequential ëª¨ë¸ì„ ë‹¤ì‹œ ìƒì„±í•œë‹¤.

```python
linear_model = tf.keras.Sequential([
    normalizer,
    layers.Dense(units=1)
])

linear_model.predict(train_features[:10])

ê²°ê³¼ê°’ : 
1/1 [==============================] - 0s 75ms/step
array([[-1.155],
       [-0.724],
       [ 0.703],
       [ 0.384],
       [ 1.115],
       [-0.033],
       [ 0.908],
       [-2.392],
       [-0.736],
       [-0.436]], dtype=float32)
```

ëª¨ë¸ì„ í˜¸ì¶œí•˜ë©´ ê°€ì¤‘ì¹˜ í–‰ë ¬ì´ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.Â `kernel`ê°€ì¤‘ì¹˜(y=mx+bì˜Â m)ê°€Â `(9, 1)`ëª¨ì–‘ì¸ì§€ í™•ì¸í•œë‹¤.

```python
linear_model.layers[1].kernel

ê²°ê³¼ê°’:
<tf.Variable 'dense_1/kernel:0' shape=(9, 1) dtype=float32, numpy=
array([[ 0.522],
       [ 0.339],
       [ 0.165],
       [-0.153],
       [-0.638],
       [ 0.728],
       [-0.507],
       [ 0.506],
       [-0.303]], dtype=float32)>
```

ìœ„ ê³¼ì •ê³¼ ì»´íŒŒì¼ì„ í•˜ê³  ë™ì¼í•˜ê²Œ ëª¨ë¸ì„ fití•œë‹¤.

```python
linear_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

%%time
history = linear_model.fit(
    train_features,
    train_labels,
    epochs=100,
    # Suppress logging.
    verbose=0,
    # Calculate validation results on 20% of the training data.
    validation_split = 0.2)
```

ì´ íšŒê·€ ëª¨ë¸ì˜ ëª¨ë“  ì…ë ¥ì„ ì‚¬ìš©í•˜ë©´ í•˜ë‚˜ì˜ ì…ë ¥ì´ ìˆëŠ”Â `horsepower_model`ë³´ë‹¤ í›¨ì”¬ ë” ë‚®ì€ í›ˆë ¨ ë° ê²€ì¦ ì˜¤ë¥˜ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.

```python
plot_loss(history)
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 4.png)

ë‚˜ì¤‘ì„ ìœ„í•´ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•œë‹¤.

```python
test_results['linear_model'] = linear_model.evaluate(
    test_features, test_labels, verbose=0)
```

## **ì‹¬ì¸µ ì‹ ê²½ë§(DNN)ì„ ì‚¬ìš©í•œ íšŒê·€**

ì´ëŸ¬í•œ ëª¨ë¸ì—ëŠ” ì„ í˜• ëª¨ë¸ë³´ë‹¤ ëª‡ ê°œì˜ ë ˆì´ì–´ê°€ ë” í¬í•¨ëœë‹¤.

- ì´ì „ê³¼ ê°™ì€ ì •ê·œí™” ë ˆì´ì–´(ë‹¨ì¼ ì…ë ¥ ëª¨ë¸ì˜ ê²½ìš°Â `horsepower_normalizer`Â ë° ë‹¤ì¤‘ ì…ë ¥ ëª¨ë¸ì˜ ê²½ìš°Â `normalizer`Â ì‚¬ìš©)
- ReLU(`relu`) í™œì„±í™” í•¨ìˆ˜ ë¹„ì„ í˜•ì„±ì´ ìˆëŠ” ë‘ ê°œì˜ ìˆ¨ê²¨ì§„ ë¹„ì„ í˜•Â `Dense`Â ë ˆì´ì–´
- ì„ í˜•Â `Dense`Â ë‹¨ì¼ ì¶œë ¥ ë ˆì´ì–´

ë‘ ëª¨ë¸ ëª¨ë‘ ë™ì¼í•œ í›ˆë ¨ ì ˆì°¨ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œÂ `compile`Â ë©”ì„œë“œëŠ” ì•„ë˜ì˜Â `build_and_compile_model`Â í•¨ìˆ˜ì— í¬í•¨ëœë‹¤.

```python
def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])

  model.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001))
  return model
```

### **DNNê³¼ ë‹¨ì¼ ì…ë ¥ì„ ì‚¬ìš©í•œ íšŒê·€**

ì…ë ¥ìœ¼ë¡œÂ `'Horsepower'`ë§Œ ì‚¬ìš©í•˜ê³  ì •ê·œí™” ë ˆì´ì–´ë¡œÂ `horsepower_normalizer`(ì•ì„œ ì •ì˜)ë¥¼ ì‚¬ìš©í•˜ì—¬ DNN ëª¨ë¸ ìƒì„±

```python
dnn_horsepower_model = build_and_compile_model(horsepower_normalizer)
```

```python
dnn_horsepower_model.summary()
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 5.png)

fitìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨

```python
%%time
history = dnn_horsepower_model.fit(
    train_features['Horsepower'],
    train_labels,
    validation_split=0.2,
    verbose=0, epochs=100)

plot_loss(history)
```

ì„ í˜• ë‹¨ì¼ ì…ë ¥Â `horsepower_model`ë³´ë‹¤ ì•½ê°„ ë” ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 6.png)

ì˜ˆì¸¡ì„Â `'Horsepower'`ì˜ í•¨ìˆ˜ë¡œ í”Œë¡œíŒ…í•˜ë©´ ì´ ëª¨ë¸ì´ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ì—ì„œ ì œê³µí•˜ëŠ” ë¹„ì„ í˜•ì„±ì„ ì–´ë–»ê²Œ ì´ìš©í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.

```python
# linspace(0, 250, 251) : 0ë¶€í„° 251 ì‚¬ì´ì— 251ê°œì˜ ê°’ì„ ëœë¤ìœ¼ë¡œ ìƒì„±
x = tf.linspace(0.0, 250, 251)
y = dnn_horsepower_model.predict(x)

plot_horsepower(x, y)
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 7.png)

ë‚˜ì¤‘ì„ ìœ„í•´ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ê²°ê³¼ë¥¼ ìˆ˜ì§‘

```python
test_results['dnn_horsepower_model'] = dnn_horsepower_model.evaluate(
    test_features['Horsepower'], test_labels,
    verbose=0)
```

### **DNN ë° ë‹¤ì¤‘ ì…ë ¥ì„ ì‚¬ìš©í•œ íšŒê·€**

ëª¨ë“  ì…ë ¥ì„ ì‚¬ìš©í•˜ì—¬ ì´ì „ ê²°ê³¼ë“¤ì„ ë°˜ë³µí•´ì¤€ë‹¤.

```python
dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 8.png)

```python
%%time
history = dnn_model.fit(
    train_features,
    train_labels,
    validation_split=0.2,
    verbose=0, epochs=100)

plot_loss(history)
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 9.png)

```python
test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)
```

## ì„±ëŠ¥

ëª¨ë“  ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì„ ê²€í† í•´ë³¸ë‹¤.

```python
pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T
```

|  | Mean absolute error [MPG] |
| --- | --- |
| horsepower_model | 3.646085 |
| linear_model | 2.497618 |
| dnn_horsepower_model | 2.928305 |
| dnn_model | 1.701235 |

## ì˜ˆì¸¡í•˜ê¸°

KerasÂ `Model.predict`ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œÂ `dnn_model`ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ì†ì‹¤ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python
test_predictions = dnn_model.predict(test_features).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [MPG]')
plt.ylabel('Predictions [MPG]')
lims = [0, 50]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 10.png)

í•©ë¦¬ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ì˜¤ë¥˜ì˜ ë¶„í¬ë„ í™•ì¸í•´ë³´ë„ë¡œê³  í•œë‹¤.

```python
error = test_predictions - test_labels
plt.hist(error, bins=25)
plt.xlabel('Prediction Error [MPG]')
_ = plt.ylabel('Count')
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 11.png)

ë‚˜ì¤‘ì— ëª¨ë¸ì„ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ëª¨ë¸ì„ ì €ì¥í•´ì£¼ì.

```python
dnn_model.save('dnn_model')

#ëª¨ë¸ ë‹¤ì‹œ í˜¸ì¶œ
reloaded = tf.keras.models.load_model('dnn_model')

test_results['reloaded'] = reloaded.evaluate(
    test_features, test_labels, verbose=0)
```

```python
pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T
```

|  | Mean absolute error [MPG] |
| --- | --- |
| horsepower_model | 3.646085 |
| linear_model | 2.497618 |
| dnn_horsepower_model | 2.928305 |
| dnn_model | 1.701235 |
| reloaded | 1.701235 |

## Regression ì‹¤ìŠµ ë°ì´í„°ì—ì„œ ë³´ê³ ì í•˜ëŠ” point

1. ì •í˜•ë°ì´í„° ì…ë ¥ì¸µ input_shape
2. ì •ê·œí™” ë ˆì´ì–´ì˜ ì‚¬ìš© => ì§ì ‘ ì •ê·œí™”í•´ë„ ëœë‹¤.
3. ì¶œë ¥ì¸µì´ ë¶„ë¥˜ì™€ ë‹¤ë¥´ê²Œ êµ¬ì„±ì´ ëœë‹¤ëŠ” ì 
4. loss ì„¤ì •ì´ ë¶„ë¥˜, íšŒê·€ì— ë”°ë¼ ë‹¤ë¥´ë‹¤.
5. ì…ë ¥ë³€ìˆ˜(í”¼ì²˜)ë¥¼ í•˜ë‚˜ë§Œ ì‚¬ìš©í–ˆì„ ë•Œë³´ë‹¤ ì—¬ëŸ¬ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì¡Œë‹¤. => ë°˜ë“œì‹œ ì—¬ëŸ¬ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤ë¼ê³  í•´ì„œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ì§€ ì•Šì„ ìˆ˜ë„ ìˆì§€ë§Œ ë„ˆë¬´ ì ì€ ë³€ìˆ˜ë¡œëŠ” ì˜ˆì¸¡ëª¨ë¸ì„ ì˜ ë§Œë“¤ê¸° ì–´ë µë‹¤ëŠ” ì ì„ ì•Œìˆ˜ ìˆìŠµë‹ˆë‹¤.

### **ğŸ¤” lossì™€ val_lossì˜ ì°¨ì´ëŠ” ë­”ê°€ìš”?**

lossëŠ” í›ˆë ¨ ì†ì‹¤ê°’, val_lossëŠ” ê²€ì¦ ì†ì‹¤ê°’. model.fitì—ì„œ validation_splitì—ì„œ ì§€ì •í•´ì¤„ ìˆ˜ ìˆìœ¼ë©°, í¼ì„¼íŠ¸ë¡œ ì§€ì •í•´ì¤€ë‹¤(ex. validation_split = 0.2 â‡’ validation setì„ 20%ë§Œí¼ ì§€ì •)

### **ğŸ¤” `dnn_model.predict(test_features).flatten()` ì˜ˆì¸¡ ê²°ê³¼ ë’¤ì— `flatten()` ì´ ìˆëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?**

2ì°¨ì›ì„ 1ì°¨ì›ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ê¸° ìœ„í•¨ì´ë‹¤. flatten()ì€ nì°¨ì›ì„ 1ì°¨ì›ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.

### ğŸ¤” **ì‹ ê²½ë§ ëª¨ë¸ ìì²´ê°€ 2ì°¨ì› ê²°ê³¼ê°€ ë‚˜ì˜¤ê¸°ë¡œ ë§Œë“¤ì–´ì§„ê±´ê°€ìš”?**

API ì—ì„œ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ì´ë¼ ì‚¬ì´í‚·ëŸ°ê³¼ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ ì›ë¦¬ëŠ” ë¹„ìŠ·í•˜ì§€ë§Œ ê¸°ëŠ¥ êµ¬í˜„ì— ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì„ ì‘ì„±í•  ë•ŒëŠ” c, c++ë¡œ ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì‘ì„±í•˜ê¸°ë„ í•©ë‹ˆë‹¤. í…ì„œí”Œë¡œ, íŒŒì´í† ì¹˜ëŠ” ë¦¬ì„œì²˜ë„ ì‚¬ìš©í•˜ê¸°ëŠ” í•˜ì§€ë§Œ í”„ë¡œë•íŠ¸ ê°œë°œ ë“±ì„ ìœ„í•œ ëª©ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¡Œê¸° ë•Œë¬¸ì— ë°‘ë°”ë‹¥ë¶€í„° ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒì— ë¹„í•´ ê°„ë‹¨í•œ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, API ë§ˆë‹¤ ê¸°ëŠ¥ì˜ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. ì‚¬ì´í‚·ëŸ°ì—ì„œ ì •í˜•ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ê³  ìˆëŠ” ê²ƒì²˜ëŸ¼ í…ì„œí”Œë¡œì—ì„œë„ ì •í˜•ë°ì´í„°, ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ë“±ì„ ì „ì²˜ë¦¬í•˜ëŠ” ê¸°ëŠ¥ë“¤ë„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ğŸ’¡ [Numpy ì ˆëŒ€ ê¸°ì´ˆ ë¬¸ì„œ](https://numpy.org/doc/stable/user/absolute_beginners.html)ë¥¼ í•˜ë£¨ì— 4~5ë°•ìŠ¤ ì •ë„ì”© TIL ì •ë¦¬í•˜ëŠ” ê²ƒì„ ì¶”ì²œ

# pima TF regression

## ë¼ì´ë¸ŒëŸ¬ë¦¬ import

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

## ë°ì´í„° í™•ì¸

```python
df = pd.read_csv("http://bit.ly/data-diabetes-csv")
df.shape

ê²°ê³¼ê°’ : (768, 9)
```

```python
df.head()
```

| Pregnancies | Glucose | BloodPressure | SkinThickness | Insulin | BMI | DiabetesPedigreeFunction | Age | Outcome |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 |
| 1 | 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 |
| 2 | 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 |
| 3 | 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 |
| 4 | 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 |

```python
# Insulin ê°’ì´ 0ë³´ë‹¤ í° ê°’ë§Œ ì‚¬ìš©í•œë‹¤.
df = df[df["Insulin"]>0].copy()
df.shape

ê²°ê³¼ê°’ : (394, 9)
```

## í•™ìŠµ ë°ì´í„° ë‚˜ëˆ„ê¸°

```python
# label_name ì„ Insulin ìœ¼ë¡œ ì„¤ì •.
label_name = "Insulin"

# train, test ë°ì´í„°ì…‹ì„ pandas ì˜ sampleì„ ì‚¬ìš©í•´ì„œ 8:2ë¡œ ë‚˜ëˆˆë‹¤.
train = df.sample(frac=0.8, random_state = 42)
test = df.drop(train.index)

train.shape, test.shape

ê²°ê³¼ê°’ : ((315, 9), (79, 9))
```

```python
# X, y set ë§Œë“¤ê¸°

X_train = train.drop(label_name, axis = 1)
y_train = train[label_name]

X_test = test.drop(label_name, axis = 1)
y_test = test[label_name]

X_train.shape, y_train.shape, X_test.shape, y_test.shape

ê²°ê³¼ê°’:
((315, 8), (315,), (79, 8), (79,))
```

## **ë”¥ëŸ¬ë‹ ë ˆì´ì–´ ë§Œë“¤ê¸°**

```python
# tensorflow ë¥¼ tfë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
import tensorflow as tf

# input_shape ê°’ì„ êµ¬í•œë‹¤. featureì˜ ìˆ˜ë¡œ ë§Œë“ ë‹¤.
input_shape = X_train.shape[1]
input_shape 
ê²°ê³¼ê°’ : 8
```

```python
# ëª¨ë¸ ë¹Œë“œ
model = tf.keras.models.Sequential([
# ì…ë ¥ ë ˆì´ì–´ë¥¼ Input ë ˆì´ì–´ë¡œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤.
  tf.keras.layers.Dense(128, input_shape=[input_shape]),
  tf.keras.layers.Dense(128, activation='selu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(128, activation='selu'),
  tf.keras.layers.Dropout(0.2),
# ëª¨ë¸ êµ¬ì„±ì€ ì „ì²´ì ìœ¼ë¡œ ë¶„ë¥˜ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ íšŒê·€ëŠ” ì¶œë ¥ì„ í•˜ë‚˜ë¡œ í•œë‹¤.
  tf.keras.layers.Dense(1)
])
```

### ëª¨ë¸ ì»´íŒŒì¼

```python
# ëª¨ë¸ì„ ì»´íŒŒì¼
optimizer = tf.keras.optimizers.RMSprop(0.001)

model.compile(optimizer=optimizer,
              loss=['mae', 'mse'], metrics = ["mae", "mse"])

# ëª¨ë¸ì„ ìš”ì•½.
model.summary()
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 12.png)

ì´ˆê¸° `model.summary()`ë¥¼ ì‚¬ìš©í•  ê²½ìš° ìœ„ ì´ë¯¸ì§€ì™€ ë‹¤ë¥´ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤. í•´ë‹¹ ì´ë¯¸ì§€ëŠ” ì—¬ëŸ¬ ë²ˆì˜ ìˆ˜ì •ì„ ê±°ì¹œ ìƒí™©ì´ê¸° ë•Œë¬¸ì´ë‹¤.

## í•™ìŠµ

```python
# ëª¨ë¸ì„ í•™ìŠµ
# í•™ìŠµê²°ê³¼ë¥¼ history ë³€ìˆ˜ì— í• ë‹¹
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)
history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, verbose = 0, callbacks = [early_stop])
```

```python
# history ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
df_hist = pd.DataFrame(history.history)
df_hist.tail(3)
```

|  | loss | mae | mse | val_loss | val_mae | val_mse |
| --- | --- | --- | --- | --- | --- | --- |
| 387 | 49.656857 | 49.656857 | 6215.549316 | 63.018036 | 63.018036 | 10140.137695 |
| 388 | 52.097633 | 52.097633 | 6365.834473 | 64.636993 | 64.636993 | 10197.075195 |
| 389 | 53.871567 | 53.871567 | 6537.537598 | 64.516808 | 64.516808 | 10913.817383 |

### í•™ìŠµê²°ê³¼ ì‹œê°í™”

```python
# í•™ìŠµê²°ê³¼ë¥¼ ì‹œê°í™”
df_hist[["loss", "val_loss"]].plot()
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 13.png)

```python
df_hist[['mae','val_mae']].plot()
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 14.png)

```python
df_hist[['mse', "val_mse"]].plot()
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 15.png)

## ì˜ˆì¸¡

í•™ìŠµí•œ ëª¨ë¸ì— X_test ë°ì´í„°ë¥¼ ë„£ì–´ ì˜ˆì¸¡ì„ ì§„í–‰í•œë‹¤.

```python
# y_pred
y_pred = model.predict(X_test)
y_pred[:5]

ê²°ê³¼ê°’:
3/3 [==============================] - 0s 6ms/step
array([[104.01402 ],
       [139.20187 ],
       [ 81.99638 ],
       [ 60.604176],
       [ 56.916958]], dtype=float32)
```

```python
# ì˜ˆì¸¡ê°’ì„ flattenì„ ì‚¬ìš©í•´ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜
y_predict = y_pred.flatten()
y_predict[:5]

ê²°ê³¼ê°’:
array([104.01402 , 139.20187 ,  81.99638 ,  60.604176,  56.916958],
      dtype=float32)
```

## í‰ê°€

ì˜ˆì¸¡í•œ ê°’ì„ ì‹¤ì œê°’ê³¼ ë¹„êµí•˜ë©° í‰ê°€ë¥¼ ì§„í–‰í•œë‹¤.

```python
# evaluateë¥¼ í†µí•´ í‰ê°€í•˜ê¸°
# evaluateê°€ ë³€ìˆ˜ì— í• ë‹¹í•˜ëŠ” ìˆœì„œëŠ” ëª¨ë¸ compile ë•Œ ì§€ì •í•´ì¤€ ìˆœì„œëŒ€ë¡œ í• ë‹¹í•´ì¤€ë‹¤.

test_loss, test_mae, test_mse = model.evaluate(X_test, y_test)
```

```python
print("í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ loss: {:5.2f}".format(test_loss))
print("í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ mae: {:5.2f}".format(test_mae))
print("í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ mse: {:5.2f}".format(test_mse))

ê²°ê³¼ê°’:
í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ loss: 63.63
í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ mae: 63.63
í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ mse: 11607.33
```

```python
# jointplot ìœ¼ë¡œ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ì‹œê°í™”
sns.jointplot(x = y_test, y = y_predict)
```

Untitled ![]({{ site.url }}{{ site.baseurl }}/assets/images/2022-11-30-deep_learning_regressor/Untitled 16.png)