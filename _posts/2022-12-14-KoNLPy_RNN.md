---
title: "KoNLPy, RNN"
excerpt: "2022-12-14 "KoNLPy, RNN"

# layout: post
categories:
  - TIL
tags:
  - python
  - Deep Learning
  - RNN
  - KoNLPy
  - IDF
  - CountVectorizer
spotifyplaylist: spotify/playlist/2KaQr0nx66AX399ZLLuTVf?si=43a48325c8fc4b16
---
### âš ï¸ í•´ë‹¹ ë‚´ìš©ì€ ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼ AI School ì˜¤ëŠ˜ì½”ë“œ ë°•ì¡°ì€ ê°•ì‚¬ì˜ ìë£Œë¥¼ í† ëŒ€ë¡œ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.

## ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ

```python
import pandas as pd
import numpy as np
```

## ë°ì´í„° ë¡œë“œ

```python
# ë°ì´ì½˜ì˜ í•´ë‹¹ ë°ì´í„°ì…‹ì€ CC-BY-4.0 ë¼ì´ì„¼ìŠ¤
# ë°ì´í„° ì¶œì²˜ : https://dacon.io/competitions/official/235747/data
# ë¡œì»¬ PCì—ì„œ ì‹¤ìŠµ ì‹œ ì§ì ‘ ë°ì´ì½˜ ì‚¬ì´íŠ¸ì— íšŒì›ê°€ì…í•˜ê³  ë‹¤ìš´ë¡œë“œ ìš”ë§

import os, platform
base_path = "data/klue/"

def file_exist_check(base_path):
    if os.path.exists(f"{base_path}train_data.csv"):
        print(f"{base_path} ê²½ë¡œì— íŒŒì¼ì´ ì´ë¯¸ ìˆìŒ")
        return
    
    if platform.system() == "Linux":
        print(f"íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í•˜ê³  {base_path} ê²½ë¡œì— ì••ì¶•ì„ í•´ì œí•¨")
        !wget https://bit.ly/dacon-klue-open-zip
        if not os.path.exists(base_path):
            os.makedirs(base_path)
        !unzip dacon-klue-open-zip -d data/klue
    else:
        print(f"""https://dacon.io/competitions/official/235747/data ì—ì„œ ë‹¤ìš´ë¡œë“œ í•˜ê³ 
              ì‹¤ìŠµ ê²½ë¡œ {base_path}ì— ì˜®ê²¨ì£¼ì„¸ìš”.""")
    return
    
file_exist_check(base_path)
```

```python
# í•™ìŠµ, ì˜ˆì¸¡ ë°ì´í„°ì…‹
train = pd.read_csv(f"{base_path}train_data.csv")
test = pd.read_csv(f"{base_path}test_data.csv")
train.shape, test.shape

out:
((45654, 3), (9131, 2))
```

```python
# í† í”½
topic = pd.read_csv(f"{base_path}topic_dict.csv")
topic
```

|  | topic | topic_idx |
| --- | --- | --- |
| 0 | ITê³¼í•™ | 0 |
| 1 | ê²½ì œ | 1 |
| 2 | ì‚¬íšŒ | 2 |
| 3 | ìƒí™œë¬¸í™” | 3 |
| 4 | ì„¸ê³„ | 4 |
| 5 | ìŠ¤í¬ì¸  | 5 |
| 6 | ì •ì¹˜ |  |

## ë¬¸ì ì „ì²˜ë¦¬

```python
# ì •ê·œí‘œí˜„ì‹
import re

def preprocessing(text):
    # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ëª¨ë‘ ì œê±°í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    text = re.sub('[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9]', ' ', text)
    # ì¤‘ë³µìœ¼ë¡œ ìƒì„±ëœ ê³µë°±ê°’ì„ ì œê±°í•©ë‹ˆë‹¤.
    text = re.sub('[\s]+', ' ', text)
    # ì˜ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë§Œë“­ë‹ˆë‹¤.
    text = text.lower()
    return text
```

```python
# !pip install tqdm --upgrade
# tqdm ìœ¼ë¡œ ì „ì²˜ë¦¬ ì§„í–‰ ìƒíƒœë¥¼ í‘œì‹œ
from tqdm import tqdm
tqdm.pandas() 

# mapì„ í†µí•´ ì „ì²˜ë¦¬ ì¼ê´„ ì ìš©
train["title"] = train["title"].progress_map(preprocessing)
test["title"] = test["title"].progress_map(preprocessing)

out:
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45654/45654 [00:00<00:00, 122698.84it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9131/9131 [00:00<00:00, 135279.58it/s]
```

## í˜•íƒœì†Œ ë¶„ì„

konlpyê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ ì„¤ì¹˜ë¥¼ ì§„í–‰í•œë‹¤. konlpyëŠ” ë‹¤ë¥¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´(JAVA, C++)ë¡œ ë§Œë“¤ì–´ì§„ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ íŒŒì´ì¬ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬ì´ë‹¤. JPype1ë„ íŒŒì´ì¬ì—ì„œ ìë°”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë„êµ¬ë‹¤. ì¸í„°í˜ì´ìŠ¤ê°€ íŒŒì´ì¬ì´ì§€ë§Œ ë‚´ë¶€ëŠ” í•´ë‹¹ ì–¸ì–´ë¡œ ë™ì‘í•˜ì—¬ ë‹¤ë¥¸ ì–¸ì–´ë„ í•¨ê»˜ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤. ê·¸ë˜ì„œ ì„¤ì¹˜ëŠ” ê¼­ ê³µì‹ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ë„ë¡ í•˜ì.

âš ï¸ì‹¤ìŠµì„ ëª©ì ìœ¼ë¡œ konlpyë¥¼ ì‚¬ìš©í•  ì˜ˆì •ì´ë¼ë©´ Colab í™˜ê²½ì—ì„œ í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤. ìœ„ ì„¤ëª…ì—ì„œë„ ì–¸ê¸‰í•˜ì˜€ì§€ë§Œ, ë‹¤ë¥¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ ë§Œë“¤ì–´ì ¸ìˆê¸° ë•Œë¬¸ì— ë¡œì»¬ íŒŒì´ì¬ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ ê½¤ë‚˜ ë²ˆê±°ë¡œìš´ ì‘ì—…ì„ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. [konlpy ê³µì‹ í™ˆí˜ì´ì§€ ì„¤ì¹˜ê°€ì´ë“œ](https://konlpy.org/ko/latest/install/)ë¥¼ ì°¸ê³ í•˜ë„ë¡ í•˜ì. ë§Œì•½, ìœˆë„ìš° í™˜ê²½ì— ì•„ë‚˜ì½˜ë‹¤ë¥¼ ì‚¬ìš©í•˜ë©´ í•´ë‹¹ [ë§í¬](https://velog.io/@soo-im/konlpy-%EC%84%A4%EC%B9%98-%EC%97%90%EB%9F%AC-%ED%95%B4%EA%B2%B0%EC%B1%85-%EC%95%84%EB%82%98%EC%BD%98%EB%8B%A4-JPYPE)ë¥¼ ì°¸ê³ í•˜ë„ë¡ í•œë‹¤.

### konlpy

```python
# colab í™˜ê²½ì—ì„œ ì„¤ì¹˜
!pip install konlpy --upgrade
```

```python
small_text = "ë²„ìŠ¤ì˜ ìš´í–‰ì‹œê°„ì„ ë¬¸ì˜í•©ë‹ˆë‹¤. ì–´?!"
small_text

out:
ë²„ìŠ¤ì˜ ìš´í–‰ì‹œê°„ì„ ë¬¸ì˜í•©ë‹ˆë‹¤. ì–´?!
```

```python
kkma.morphs(u'ê³µë¶€ë¥¼ í•˜ë©´ í• ìˆ˜ë¡ ëª¨ë¥´ëŠ”ê²ƒì´ ë§ë‹¤ëŠ”ê²ƒì„ ì•Œê²Œ ë©ë‹ˆë‹¤.')

out:
['ê³µë¶€',
 'ë¥¼',
 'í•˜',
 'ë©´',
 'í•˜',
 'ã„¹ìˆ˜ë¡',
 'ëª¨ë¥´',
 'ëŠ”',
 'ê²ƒ',
 'ì´',
 'ë§',
 'ë‹¤ëŠ”',
 'ê²ƒ',
 'ì„',
 'ì•Œ',
 'ê²Œ',
 'ë˜',
 'ã…‚ë‹ˆë‹¤',
 '.']
```

```python
kkma.pos(u'ê³µë¶€ë¥¼ í•˜ë©´ í• ìˆ˜ë¡ ëª¨ë¥´ëŠ”ê²ƒì´ ë§ë‹¤ëŠ”ê²ƒì„ ì•Œê²Œ ë©ë‹ˆë‹¤.')

out:
[('ê³µë¶€', 'NNG'),
 ('ë¥¼', 'JKO'),
 ('í•˜', 'VV'),
 ('ë©´', 'ECE'),
 ('í•˜', 'VV'),
 ('ã„¹ìˆ˜ë¡', 'ECD'),
 ('ëª¨ë¥´', 'VV'),
 ('ëŠ”', 'ETD'),
 ('ê²ƒ', 'NNB'),
 ('ì´', 'JKS'),
 ('ë§', 'VA'),
 ('ë‹¤ëŠ”', 'ETD'),
 ('ê²ƒ', 'NNB'),
 ('ì„', 'JKO'),
 ('ì•Œ', 'VV'),
 ('ê²Œ', 'ECD'),
 ('ë˜', 'VV'),
 ('ã…‚ë‹ˆë‹¤', 'EFN'),
 ('.', 'SF')]
```

### Pecab

```python
!pip install pecab
```

```python
from pecab import PeCab

pecab = PeCab()
pecab.pos("ì €ëŠ” ì‚¼ì„±ë””ì§€í„¸í”„ë¼ìì—ì„œ ì§€í ëƒ‰ì¥ê³ ë¥¼ ìƒ€ì–´ìš”.")

out:
[('ì €', 'NP'),
 ('ëŠ”', 'JX'),
 ('ì‚¼ì„±', 'NNP'),
 ('ë””ì§€í„¸', 'NNP'),
 ('í”„ë¼ì', 'NNP'),
 ('ì—ì„œ', 'JKB'),
 ('ì§€', 'NNP'),
 ('í ', 'NNP'),
 ('ëƒ‰ì¥ê³ ', 'NNG'),
 ('ë¥¼', 'JKO'),
 ('ìƒ€', 'VV+EP'),
 ('ì–´ìš”', 'EF'),
 ('.', 'SF')]
```

### [Stemming(ì–´ê°„ ì¶”ì¶œ)](https://ko.wikipedia.org/wiki/%EC%96%B4%EA%B0%84_%EC%B6%94%EC%B6%9C)

í˜•íƒœë¡  ë° ì •ë³´ ê²€ìƒ‰ ë¶„ì•¼ì—ì„œ ì–´í˜•ì´ ë³€í˜•ëœ ë‹¨ì–´ë¡œë¶€í„° ì ‘ì‚¬ ë“±ì„ ì œê±°í•˜ê³  ê·¸ ë‹¨ì–´ì˜ ì–´ê°„ì„ ë¶„ë¦¬í•´ ë‚´ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì—¬ê¸°ì„œ ì–´ê°„ì€ ë°˜ë“œì‹œ ì–´ê·¼ê³¼ ê°™ì•„ì•¼ í•  í•„ìš”ëŠ” ì—†ìœ¼ë©°, ì–´ê·¼ê³¼ ì°¨ì´ê°€ ìˆë”ë¼ë„ ê´€ë ¨ì´ ìˆëŠ” ë‹¨ì–´ë“¤ì´ ì¼ì •í•˜ê²Œ ë™ì¼í•œ ì–´ê°„ìœ¼ë¡œ ë§µí•‘ë˜ê²Œ í•˜ëŠ” ê²ƒì´ ì–´ê°„ ì¶”ì¶œì˜ ëª©ì ì´ë‹¤. 1960ë…„ëŒ€ë¶€í„° ì»´í“¨í„° ê³¼í•™ ë¶„ì•¼ì—ì„œ ë‹¤ì–‘í•œ ì–´ê°„ ì¶”ì¶œ ê´€ë ¨ ì•Œê³ ë¦¬ì¦˜ë“¤ì´ ì—°êµ¬ë˜ì–´ ì™”ë‹¤. ë§ì€ ì›¹ ê²€ìƒ‰ ì—”ì§„ë“¤ì€ ë™ì¼í•œ ì–´ê°„ì„ ê°€ì§„ ë‹¨ì–´ë“¤ì„ ë™ì˜ì–´ë¡œ ì·¨ê¸‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§ˆì˜ì–´ í™•ì¥ì„ í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ ë†’ì¸ë‹¤.
ì–´ê°„ ì¶”ì¶œ í”„ë¡œê·¸ë¨ì€ í”íˆ ìŠ¤í…Œë° ì•Œê³ ë¦¬ì¦˜(stemming algorithm) ë˜ëŠ” ìŠ¤í…Œë¨¸(stemmer)ë¼ ë¶ˆë¦°ë‹¤.

```python
# Okt
# steming ê¸°ëŠ¥ì„ ì œê³µ
from konlpy.tag import Okt

okt = Okt()
okt.pos(small_text)

out:
[('ë²„ìŠ¤', 'Noun'),
 ('ì˜', 'Josa'),
 ('ìš´í–‰', 'Noun'),
 ('ì‹œê°„', 'Noun'),
 ('ì„', 'Josa'),
 ('ë¬¸ì˜', 'Noun'),
 ('í•©ë‹ˆë‹¤', 'Verb'),
 ('.', 'Punctuation'),
 ('ì–´', 'Eomi'),
 ('?!', 'Punctuation')]
```

```python
okt.pos(small_text, stem = True)

out:
[('ë²„ìŠ¤', 'Noun'),
 ('ì˜', 'Josa'),
 ('ìš´í–‰', 'Noun'),
 ('ì‹œê°„', 'Noun'),
 ('ì„', 'Josa'),
 ('ë¬¸ì˜', 'Noun'),
 ('í•˜ë‹¤', 'Verb'),
 ('.', 'Punctuation'),
 ('ì–´', 'Eomi'),
 ('?!', 'Punctuation')]
```

í˜•íƒœì†Œ ë¶„ì„ê¸°(Okt) ë¶ˆëŸ¬ì˜¤ê¸° 
['Josa', 'Eomi', 'Punctuation'] : ì¡°ì‚¬, ì–´ë¯¸, êµ¬ë‘ì  ì œê±°
ì „ì²´ í…ìŠ¤íŠ¸ì— ì ìš©í•´ ì£¼ê¸° ìœ„í•´ í•¨ìˆ˜ë¥¼ ë§Œë“ ë‹¤.
1) í…ìŠ¤íŠ¸ ì…ë ¥ë°›ê¸°
2) í’ˆì‚¬íƒœê¹… [('ë¬¸ì˜', 'Noun'), ('í•˜ë‹¤', 'Verb'), ('?!', 'Punctuation')]
3) íƒœê¹… ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ìˆœíšŒ
4) í•˜ë‚˜ì”© ìˆœíšŒ í–ˆì„ ë•Œ íŠœí”Œ í˜•íƒœë¡œ ê°€ì ¸ì˜¤ê²Œ ëœë‹¤. ('ì„', 'Josa') 
5) íŠœí”Œì—ì„œ 1ë²ˆ ì¸ë±ìŠ¤ì— ìˆëŠ” í’ˆì‚¬ë¥¼ ê°€ì ¸ì˜¨ë‹¤.
6) í•´ë‹¹ í’ˆì‚¬ê°€ ì¡°ì‚¬, ì–´ë¯¸, êµ¬ë‘ì ì´ë©´ ì œì™¸í•˜ê³  append ë¡œ ì¸ë±ìŠ¤ 0ë²ˆ ê°’ë§Œ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ì¤€ë‹¤.
7) " ".join() ìœ¼ë¡œ ê³µë°±ë¬¸ìë¡œ ì—°ê²°í•´ ì£¼ë©´ ë‹¤ì‹œ ë¬¸ì¥ì´ ì™„ì„±ëœë‹¤.
8) ì „ì²˜ë¦¬ í›„ ì™„ì„±ëœ ë¬¸ì¥ì„ ë°˜í™˜í•´ì¤€ë‹¤.

```python
def okt_clean(text):
    clean_text = []
    # í’ˆì‚¬íƒœê¹…ì„ í•©ë‹ˆë‹¤. [('ë¬¸ì˜', 'Noun'), ('í•˜ë‹¤', 'Verb'), ('?!', 'Punctuation')]
    # íƒœê¹… ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ìˆœíšŒ í•©ë‹ˆë‹¤. 
    for word in okt.pos(text, norm=True, stem=True):
        # í•´ë‹¹ í’ˆì‚¬ê°€ ì¡°ì‚¬, ì–´ë¯¸, êµ¬ë‘ì ì´ë©´ ì œì™¸í•˜ê³  append ë¡œ ì¸ë±ìŠ¤ 0ë²ˆ ê°’ë§Œ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ì¤ë‹ˆë‹¤.
        if word[1] not in ['Josa', 'Eomi', 'Punctuation']:
            clean_text.append(word[0])
    # " ".join() ìœ¼ë¡œ ê³µë°±ë¬¸ìë¡œ ì—°ê²°í•´ ì£¼ë©´ ë‹¤ì‹œ ë¬¸ì¥ì´ ë©ë‹ˆë‹¤.
    return " ".join(clean_text)

okt_clean("ë²„ìŠ¤ ìš´í–‰ì‹œê°„ì„ í–ˆì—ˆë„¤?")

out:
ë²„ìŠ¤ ìš´í–‰ ì‹œê°„ í•˜ë‹¤
```

```python
train['title'] = train['title'].progress_map(preprocessing)
test['title'] = test['title'].progress_map(preprocessing)

train['title'] = train['title'].progress_map(okt_clean)
test['title'] = test['title'].progress_map(okt_clean)
```

## ë¶ˆìš©ì–´ ì œê±°

```python
# ë¶ˆìš©ì–´ ì œê±°
def remove_stopwords(text):
    tokens = text.split(' ')
    stops = [ 'í•©ë‹ˆë‹¤', 'í•˜ëŠ”', 'í• ', 'í•˜ê³ ', 'í•œë‹¤', 
             'ê·¸ë¦¬ê³ ', 'ì…ë‹ˆë‹¤', 'ê·¸', 'ë“±', 'ì´ëŸ°', 'ë°','ì œ', 'ë”']
    meaningful_words = [w for w in tokens if not w in stops]
    return ' '.join(meaningful_words)
```

```python
train["title"] = train["title"].map(remove_stopwords)
test["title"] = test["title"].map(remove_stopwords)
```

## í•™ìŠµ, ì˜ˆì¸¡ ë°ì´í„° ë§Œë“¤ê¸°

```python
X_train_text = train["title"]
X_test_text = test["title"]

label_name = "topic_idx"

y_train = train[label_name]
```

## ë²¡í„°í™”

### TF-IDF

```python
from sklearn.feature_extraction.text import TfidfVectorizer

tfidfvect = TfidfVectorizer()
tfidfvect.fit(X_train_text)
```

```python
# transform : ì—´(columns, ì–´íœ˜)ì˜ ìˆ˜ê°€ ê°™ì€ì§€ í™•ì¸í•´ë³¼ ê²ƒ
X_train = tfidfvect.transform(X_train_text)
X_test = tfidfvect.transform(X_test_text)

X_train.shape, X_test.shape

out: ((45654, 28605), (9131, 28605))
```

## ëª¨ë¸ ì„¤ì •ê³¼ í•™ìŠµ ë° ì˜ˆì¸¡ê³¼ ì œì¶œ

```python
# ëª¨ë¸
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(random_state = 42)

# CV
from sklearn.model_selection import cross_val_predict

y_predict = cross_val_predict(model, X_train, y_train, cv = 3, n_jobs = -1, verbose = 1)

score = (y_train == y_predict).mean()
score

out:
0.6957112191702808

# ì˜ˆì¸¡
y_test_predict = model.fit(X_train, y_train).predict(X_test)

# ì˜ˆì¸¡ csvì— ê°’ ë„£ê¸°
submit = pd.read_csv(f"{base_path}sample_submission.csv")

submit["topic_idx"] = y_test_predict

file_name = f"{base_path}submit_{score}.csv"

submit.to_csv(file_name, index = False)
```

# ì‹œí€€ìŠ¤ ì¸ì½”ë”©

## Tokenizer

```python
import pandas as pd

corpus = ["ì„œìš¸ ì½”ë¡œë‚˜ ìƒìƒì§€ì›ê¸ˆ ë¬¸ì˜ì…ë‹ˆë‹¤.?",
"ì¸ì²œ ì§€í•˜ì²  ìš´í–‰ì‹œê°„ ë¬¸ì˜ì…ë‹ˆë‹¤.!",
"ë²„ìŠ¤ ìš´í–‰ì‹œê°„ ë¬¸ì˜ì…ë‹ˆë‹¤.#"]
```

TokenizerëŠ” ë°ì´í„°ì— ì¶œí˜„í•˜ëŠ” ëª¨ë“  ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì„¸ê³  ë¹ˆë„ ìˆ˜ë¡œ ì •ë ¬í•´ì„œ num_words ì— ì§€ì •ëœ ë§Œí¼ë§Œ ìˆ«ìë¡œ ë°˜í™˜í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” 0 ìœ¼ë¡œ ë°˜í™˜í•œë‹¤. ë‹¨ì–´ ì‚¬ì „ì˜ í¬ê¸°ë¥¼ ì§€ì •í•´ ì£¼ê¸° ìœ„í•´ vocab_sizeë¥¼ ì§€ì •í•´ì¤€ë‹¤. vocab_sizeëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ ëœ»í•œë‹¤.

```python
from tensorflow.keras.preprocessing.text import Tokenizer

vocab_size = 5  
tokenizer = Tokenizer(num_words = vocab_size)
```

```python
# Tokenizer ì— ë°ì´í„° ì‹¤ì œë¡œ ì…ë ¥
# fit_on_textsì™€ word_indexë¥¼ ì‚¬ìš©í•˜ì—¬ key valueë¡œ ì´ë£¨ì–´ì§„ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±
tokenizer.fit_on_texts(corpus)
```

```python
# tokenizerì˜ word_index ì†ì„±ì€ ë‹¨ì–´ì™€ ìˆ«ìì˜ í‚¤-ê°’ ìŒì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜
# ì´ë•Œ, ë°˜í™˜ ì‹œ ìë™ìœ¼ë¡œ ì†Œë¬¸ìë¡œ ë³€í™˜ë˜ì–´ ë“¤ì–´ê°€ë©°, ëŠë‚Œí‘œë‚˜ ë§ˆì¹¨í‘œ ê°™ì€ êµ¬ë‘ì ì€ ìë™ìœ¼ë¡œ ì œê±°.

word_to_index = tokenizer.word_index
word_to_index

out:
{'ë¬¸ì˜ì…ë‹ˆë‹¤': 1,
 'ìš´í–‰ì‹œê°„': 2,
 'ì„œìš¸': 3,
 'ì½”ë¡œë‚˜': 4,
 'ìƒìƒì§€ì›ê¸ˆ': 5,
 'ì¸ì²œ': 6,
 'ì§€í•˜ì² ': 7,
 'ë²„ìŠ¤': 8}
```

```python
word_to_index.values()

out:
dict_values([1, 2, 3, 4, 5, 6, 7, 8])
```

```python
# dfë¡œ ë³€í™˜
wc = tokenizer.word_counts
pd.DataFrame(wc.items()).set_index(0).T
```

|  | ì„œìš¸ | ì½”ë¡œë‚˜ | ìƒìƒì§€ì›ê¸ˆ | ë¬¸ì˜ì…ë‹ˆë‹¤ | ì¸ì²œ | ì§€í•˜ì²  | ìš´í–‰ì‹œê°„ | ë²„ìŠ¤ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | 1 | 1 | 1 | 3 | 1 | 1 | 2 | 1 |

```python
# texts_to_sequencesë¥¼ ì´ìš©í•˜ì—¬ text ë¬¸ì¥ì„ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€ê²½
# BOWëŠ” ë“±ì¥ìœ ë¬´ë¥¼ ë³´ì•˜ë‹¤ë©´, ì‹œí€€ìŠ¤ ë°©ì‹ì€ í•´ë‹¹ ì–´íœ˜ì‚¬ì „ì„ ë§Œë“¤ê³  ì–´íœ˜ì˜ ë“±ì¥ ìˆœì„œëŒ€ë¡œ ìˆ«ìë¡œ ë³€í™˜

corpus_sequences = tokenizer.texts_to_sequences(corpus)
corpus_sequences

out:
[[3, 4, 1], [2, 1], [2, 1]]
```

```python
# ovv(out of vocab)
# ovv_tokenì— ê¼­ <oov>ê°€ ë“¤ì–´ê°ˆ í•„ìš”ëŠ” ì—†ë‹¤

tokenizer = Tokenizer(num_words= 10, oov_token="<oov>")
tokenizer.fit_on_texts(corpus)
print(tokenizer.word_index)
print(corpus)
corpus_sequences = tokenizer.texts_to_sequences(corpus)
corpus_sequences

out:
{'<ovv>': 1, 'ë¬¸ì˜ì…ë‹ˆë‹¤': 2, 'ìš´í–‰ì‹œê°„': 3, 'ì„œìš¸': 4, 'ì½”ë¡œë‚˜': 5, 'ìƒìƒì§€ì›ê¸ˆ': 6, 'ì¸ì²œ': 7, 'ì§€í•˜ì² ': 8, 'ë²„ìŠ¤': 9}
['ì„œìš¸ ì½”ë¡œë‚˜ ìƒìƒì§€ì›ê¸ˆ ë¬¸ì˜ì…ë‹ˆë‹¤.?', 'ì¸ì²œ ì§€í•˜ì²  ìš´í–‰ì‹œê°„ ë¬¸ì˜ì…ë‹ˆë‹¤.!', 'ë²„ìŠ¤ ìš´í–‰ì‹œê°„ ë¬¸ì˜ì…ë‹ˆë‹¤.#']
[[4, 5, 6, 2], [7, 8, 3, 2], [9, 3, 2]]
```

## Padding

ìì—°ì–´ ì²˜ë¦¬ë¥¼ í•˜ë‹¤ë³´ë©´ ê° ë¬¸ì¥(ë˜ëŠ” ë¬¸ì„œ)ì€ ì„œë¡œ ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤. ê¸°ê³„ëŠ” ê¸¸ì´ê°€ ì „ë¶€ ë™ì¼í•œ ë¬¸ì„œë“¤ì— ëŒ€í•´ì„œëŠ” í•˜ë‚˜ì˜ í–‰ë ¬ë¡œ ë³´ê³ , í•œêº¼ë²ˆì— ë¬¶ì–´ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•´ì„œ ì—¬ëŸ¬ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ì„ì˜ë¡œ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ëŠ” ì‘ì—…ì´ í•„ìš”í•  ë•Œê°€ ìˆë‹¤.

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

import numpy as np

pads = pad_sequences(corpus_sequences, maxlen = 10)
print(corpus)
print(word_to_index)
print(pads)
np.array(pads)

out:
['ì„œìš¸ ì½”ë¡œë‚˜ ìƒìƒì§€ì›ê¸ˆ ë¬¸ì˜ì…ë‹ˆë‹¤.?', 'ì¸ì²œ ì§€í•˜ì²  ìš´í–‰ì‹œê°„ ë¬¸ì˜ì…ë‹ˆë‹¤.!', 'ë²„ìŠ¤ ìš´í–‰ì‹œê°„ ë¬¸ì˜ì…ë‹ˆë‹¤.#']
{'ë¬¸ì˜ì…ë‹ˆë‹¤': 1, 'ìš´í–‰ì‹œê°„': 2, 'ì„œìš¸': 3, 'ì½”ë¡œë‚˜': 4, 'ìƒìƒì§€ì›ê¸ˆ': 5, 'ì¸ì²œ': 6, 'ì§€í•˜ì² ': 7, 'ë²„ìŠ¤': 8}
[[0 0 0 0 0 0 4 5 6 2]
 [0 0 0 0 0 0 7 8 3 2]
 [0 0 0 0 0 0 0 9 3 2]]
array([[0, 0, 0, 0, 0, 0, 4, 5, 6, 2],
       [0, 0, 0, 0, 0, 0, 7, 8, 3, 2],
       [0, 0, 0, 0, 0, 0, 0, 9, 3, 2]])
```

# RNN(Recurrent Neural Network)ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¥˜í•˜ê¸°

- [ìˆœí™˜ ì‹ ê²½ë§(Recurrent neural network, RNN)](https://ko.wikipedia.org/wiki/%EC%88%9C%ED%99%98_%EC%8B%A0%EA%B2%BD%EB%A7%9D)ì€ ì¸ê³µ ì‹ ê²½ë§ì˜ í•œ ì¢…ë¥˜ë¡œ, ìœ ë‹›ê°„ì˜ ì—°ê²°ì´ ìˆœí™˜ì  êµ¬ì¡°ë¥¼ ê°–ëŠ” íŠ¹ì§•ì„ ê°–ê³  ìˆë‹¤. ì´ëŸ¬í•œ êµ¬ì¡°ëŠ” ì‹œë³€ì  ë™ì  íŠ¹ì§•ì„ ëª¨ë¸ë§ í•  ìˆ˜ ìˆë„ë¡ ì‹ ê²½ë§ ë‚´ë¶€ì— ìƒíƒœë¥¼ ì €ì¥í•  ìˆ˜ ìˆê²Œ í•´ì£¼ë¯€ë¡œ, ìˆœë°©í–¥ ì‹ ê²½ë§ê³¼ ë‹¬ë¦¬ ë‚´ë¶€ì˜ ë©”ëª¨ë¦¬ë¥¼ ì´ìš©í•´ ì‹œí€€ìŠ¤ í˜•íƒœì˜ ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ìˆœí™˜ ì¸ê³µ ì‹ ê²½ë§ì€ í•„ê¸° ì¸ì‹ì´ë‚˜ ìŒì„± ì¸ì‹ê³¼ ê°™ì´ ì‹œë³€ì  íŠ¹ì§•ì„ ì§€ë‹ˆëŠ” ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ”ë° ì ìš©í•  ìˆ˜ ìˆë‹¤.
- ìˆœí™˜ ì‹ ê²½ë§ì´ë¼ëŠ” ì´ë¦„ì€ ì…ë ¥ë°›ëŠ” ì‹ í˜¸ì˜ ê¸¸ì´ê°€ í•œì •ë˜ì§€ ì•Šì€ ë™ì  ë°ì´í„°ë¥¼ ì²˜ë¦¬í•œë‹¤ëŠ” ì ì—ì„œ ë¶™ì—¬ì§„ ì´ë¦„ìœ¼ë¡œ, ìœ í•œ ì„í„ìŠ¤ êµ¬ì¡°ì™€ ë¬´í•œ ì„í„ìŠ¤ êµ¬ì¡°ë¥¼ ëª¨ë‘ ì¼ì»«ëŠ”ë‹¤. ìœ í•œ ì„í„ìŠ¤ ìˆœí™˜ ì‹ ê²½ë§ì€ ìœ í–¥ ë¹„ìˆœí™˜ ê·¸ë˜í”„ì´ë¯€ë¡œ ì ì ˆí•˜ê²Œ í’€ì–´ì„œ ì¬êµ¬ì„±í•œë‹¤ë©´ ìˆœë°©í–¥ ì‹ ê²½ë§ìœ¼ë¡œë„ í‘œí˜„í•  ìˆ˜ ìˆì§€ë§Œ, ë¬´í•œ ì„í„ìŠ¤ ìˆœí™˜ ì‹ ê²½ë§ì€ ìœ í–¥ ê·¸ë˜í”„ì¸ê³ ë¡œ ìˆœë°©í–¥ ì‹ ê²½ë§ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.
- ìˆœí™˜ ì‹ ê²½ë§ì€ ì¶”ê°€ì ì¸ ì €ì¥ê³µê°„ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ì´ ì €ì¥ê³µê°„ì´ ê·¸ë˜í”„ì˜ í˜•íƒœë¥¼ ê°€ì§ìœ¼ë¡œì¨ ì‹œê°„ ì§€ì—°ì˜ ê¸°ëŠ¥ì„ í•˜ê±°ë‚˜ í”¼ë“œë°± ë£¨í”„ë¥¼ ê°€ì§ˆ ìˆ˜ë„ ìˆë‹¤. ì´ì™€ ê°™ì€ ì €ì¥ê³µê°„ì„ ê²Œì´íŠ¸ëœ ìƒíƒœ(gated state) ë˜ëŠ” ê²Œì´íŠ¸ëœ ë©”ëª¨ë¦¬(gated memory)ë¼ê³  í•˜ë©°, LSTMê³¼ ê²Œì´íŠ¸ ìˆœí™˜ ìœ ë‹›(GRU)ì´ ì´ë¥¼ ì‘ìš©í•˜ëŠ” ëŒ€í‘œì ì¸ ì˜ˆì‹œì´ë‹¤.

## ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```

## ë°ì´í„° ë¡œë“œ ë° ìš”ì•½

```python
df = pd.read_csv("https://bit.ly/seoul-120-text-csv")
df.shape

out:
(2645, 5)

# ë¬¸ì„œë¼ëŠ” íŒŒìƒë³€ìˆ˜ ìƒì„±
df["ë¬¸ì„œ"] = df["ì œëª©"] + " " + df["ë‚´ìš©"]

df["ë¶„ë¥˜"].value_counts()

out:
í–‰ì •        1098
ê²½ì œ         823
ë³µì§€         217
í™˜ê²½         124
ì£¼íƒë„ì‹œê³„íš     110
ë¬¸í™”ê´€ê´‘        96
êµí†µ          90
ì•ˆì „          51
ê±´ê°•          23
ì—¬ì„±ê°€ì¡±        13
Name: ë¶„ë¥˜, dtype: int64
```

ë¶„ë¥˜ë³„ ë¹ˆë„ìˆ˜ ê°’ì˜ ë¶ˆê· í˜•ì´ ì‹¬í•´ ë°ì´í„° ì˜ˆì¸¡ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤. ì¼ë¶€ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤.

```python
df = df[df["ë¶„ë¥˜"].isin(["í–‰ì •","ê²½ì œ","ë³µì§€"])]
df.shape

out:
(2138, 6)
```

ì •ë‹µ ë ˆì´ë¸” ì„¤ì •ê³¼ X,y ê°’ì„ ë§Œë“¤ì–´ì¤€ë‹¤

```python
label_name = "ë¶„ë¥˜"
X, y = df["ë¬¸ì„œ"], df[label_name]

X.shape, y.shape

out:
((2138,), (2138,))
```

## Label One-Hot-Encoding

RNN ëª¨ë¸ì„ ë§Œë“¤ ì˜ˆì •ì´ë©° ì¶œë ¥ì¸µì€ ê¸°ì¡´ì— ë§Œë“¤ì—ˆë˜ ê²ƒì²˜ëŸ¼ ë§Œë“¤ ì˜ˆì •ì´ë‹¤. 

**ğŸ¤”"í–‰ì •", "ê²½ì œ", "ë³µì§€" labelì„ one-hot-encoding ì„ í•´ì£¼ëŠ” ì´ìœ ?**
ë¶„ë¥˜ ëª¨ë¸ì˜ ì¶œë ¥ì¸µì„ softmaxë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. softmaxëŠ” ê° í´ë˜ìŠ¤ì˜ í™•ë¥ ê°’ì„ ë°˜í™˜í•˜ë©° ê°ê°ì˜ í´ë˜ìŠ¤ì˜ í•©ê³„ë¥¼ êµ¬í–ˆì„ ë•Œ 1ì´ ëœë‹¤.

```python
y_onehot = pd.get_dummies(y)
y_onehot.head(2)
```

|  | ê²½ì œ | ë³µì§€ | í–‰ì • |
| --- | --- | --- | --- |
| 0 | 0 | 1 | 0 |
| 1 | 1 | 0 | 0 |

```python
# train_test_split ìœ¼ë¡œ í•™ìŠµê³¼ ì˜ˆì¸¡ì— ì‚¬ìš©í•  ë°ì´í„° ë‚˜ëˆ„ê¸°

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, stratify=y_onehot, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

out:
((1710,), (428,), (1710, 3), (428, 3))
```

```python
display(y_train.value_counts(normalize=True))
display(y_test.value_counts(normalize=True))

out:
ê²½ì œ  ë³µì§€  í–‰ì •
0   0   1     0.513450
1   0   0     0.384795
0   1   0     0.101754
dtype: float64

ê²½ì œ  ë³µì§€  í–‰ì •
0   0   1     0.514019
1   0   0     0.385514
0   1   0     0.100467
dtype: float64
```

## Vectorization

### Tokenizer

```python
from tensorflow.keras.preprocessing.text import Tokenizer
```

`Tokenizer`ëŠ” ë°ì´í„°ì— ì¶œí˜„í•˜ëŠ” ëª¨ë“  ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì„¸ê³  ë¹ˆë„ ìˆ˜ë¡œ ì •ë ¬í•´ì„œ `num_words`ì— ì§€ì •ëœ ë§Œí¼ë§Œ ìˆ«ìë¡œ ë°˜í™˜í•˜ê³ ,ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë°˜í™˜í•œë‹¤. ë‹¨ì–´ ì‚¬ì „ì˜ í¬ê¸°ë¥¼ ì§€ì •í•´ ì£¼ê¸° ìœ„í•´ `vocab_size`(í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°)ë¥¼ ì§€ì •í•´ì¤€ë‹¤.

```python
vocab_size = 1000
oov_tok = "<oov>"
tokenizer = Tokenizer(oov_token = oov_tok)

# Tokenizer ì— ë°ì´í„° ì‹¤ì œë¡œ ì…ë ¥
# fit_on_textsì™€ word_indexë¥¼ ì‚¬ìš©í•˜ì—¬ key valueë¡œ ì´ë£¨ì–´ì§„ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±
tokenizer.fit_on_texts(X_train)
```

```python
# tokenizerì˜ word_index ì†ì„±ì€ ë‹¨ì–´ì™€ ìˆ«ìì˜ í‚¤-ê°’ ìŒì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜. 
# ì´ë•Œ, ë°˜í™˜ ì‹œ ìë™ìœ¼ë¡œ ì†Œë¬¸ìë¡œ ë³€í™˜ë˜ì–´ ë“¤ì–´ê°€ë©°, ëŠë‚Œí‘œë‚˜ ë§ˆì¹¨í‘œ ê°™ì€ êµ¬ë‘ì ì€ ìë™ìœ¼ë¡œ ì œê±°.

pd.DataFrame(tokenizer.word_counts.items()).sort_values(1, ascending= False)
```

| 0 | 1 |
| --- | --- |
| 242 | ë° |
| 1338 | ë‹ì›€ |
| 73 | ìˆ˜ |
| 458 | ê²½ìš° |
| 203 | ë˜ëŠ” |
| ... | ... |
| 16459 | ë³‘ê³¼í•  |
| 16458 | í˜•ë²Œì„ |
| 16457 | ê³¼í•˜ëŠ” |
| 16456 | ê²½ìš°ì§•ê³„ë²Œì„ |
| 36256 | ì„¤ì¹˜í•˜ëŠ”ê°€ |

36257 rows Ã— 2 columns